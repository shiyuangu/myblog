* Spark
** Motivation 
MapReduce relies on disk to back up data for fault tolerance. Spark takes the view of functional programming, dataset is just a series of functional transformation from one to another. Spark keeps track of the operations for fault tolerance. 

** RDD:
*** RDD looks like Scala parallel collections but with added info for distributed computation. 
*** Action vs. Transformation
+ Transformation returns RDD, is *lazy* (vs. /transformer/ in scala)
+ Compute a result based on an RDD, either returned or saved to an external storage system(eg. HDFS). Action is *eager*
+ To determine whether a function is a transformation or an action, we can look at the return type. A transformation always return another RDD while action always return non-RDD. 
+ Laziness/eagerness is how we can limit network communicating using the programming model 
#+BEGIN_SRC scala
val people: RDD[Person] = ...
people.foreach(println) 
#What happen on the driver when we run the foreach command. Noting happens. Why? RDD.foreach is an action. It prints to stdout of the executors but println return Unit.
#+END_SRC

**** Transformations 
+ Common transformations (note that all return RDD) 
//scala has similar such operations
map[B](f: A=>B): RDD[B]
flatMap[B](f: A=> TraversableOnce[B]): RDD[B]
filter(pred: A=>Boolean): RDD[A]
distinct(): RDD[B]


//set operations
union(other: RDD[T]): RDD[T]
intersection(other:RDD[T]): RDD[T]
subtract(other:RDD[T]): RDD[T]
cartesian[U](other: RDD[U]): RDD[(T,U)]


**** Actions

+ /reduce()/, /fold()/
  return type has to be the element type of RDD
+ /aggregate()/
  can be any return type; caller needs to provides two functions, one for reduction, one for merging two accumulators, given that each node accumulates its own results locally 

+ Use /foreach/ if no value needs to return. 

Q: What's the difference between /fold()/ vs. /reduce()/
Practical difference between these methods in Spark is only related to their behavior on empty collections and ability to use mutable buffer (arguably it is related to performance)
http://stackoverflow.com/questions/36056895/reduce-vs-fold-in-apache-spark
http://stackoverflow.com/questions/34529953/why-is-the-fold-action-necessary-in-spark

+ Common Actions(note that actions always return Non-RDD)
collect(): Array[T]
count(): Long
take(num:Int): Array[T]
reduce(op:(A,A) => A): A
foreach(f: T=>Unit): Unit 

+ the following actions have no analogue in scala collections but are specific to Spark when dealing with distributed data.
#+BEGIN_SRC 
takeSample(withRepl: Boolean, sum:Int):Array[T]
takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]: return the first n elements of the RDD using either their natural order ir a custom comparator(?)
saveAsTextFile(path:String): Unit 
saveAsSequenceFile(path:String): Unit (write to Hadoop SequenceFile)
#+END_SRC 


+ Different from scala collection.  
Spark doesn't allows foldLeft/foldRight like regular scala collection. Recall foldLeft and foldRight are not parallelizable. Use aggregation instead.  

**** Performance Implication 
Laziness/eagerness of transformation/action allows Spark to optimize the *chain of operations* before executing it. In the following example, Spark see the action is take(10) and will stop filtering once there are ten records. 
#+BEGIN_SRC scala
val lastYearsLogs RDD[String] = 
val firstLogsWithErrors = lastYearsLogs.filter(_.contains("ERROR")).take(10)
#+END_SRC

Spark can also fuse multiple transformation into one which is very different than Scala collection. In the following example, spark fuse map and filter so that it only need traverse the dataset one 
#+BEGIN_SRC scala
val lastYearsLogs: RDD[String] = 
val numErros = lastYearsLogs.map(_.lowercase).filter(_.contains("error")).count()

#+END_SRC

*** Specialized RDD: DoubleRDD, PairRDD
**** DoubleRDD
+ More functions are available for JavaDoubleRDD. We can use /rdd.mapToDouble(..)/ to get a JavaDoubleRDD so that /mean()/,/variance()/ is available.  
+ In Scala, Implicit conversion happens to convert RDD<Double> to DoubleRDD.  

**** PairRDD
compare with KeyValueGroupedDataset. 
***** groupByKey
Spark: def groupByKey(): RDD[(K,Iterable[V])]
vs. 
Scala Collection: def groupBy[K](f: A=>K): Map[K, Traversable[A]]

Note that groupByKey() in Spark returns another RDD hence, groupByKey() is a transformation.
***** reduceByKey
def reduceByKey(f: (V,V) =>V): RDD[(K,V)]
reduceByKey could be more efficient than groupByKey since it reduces the amount of data requiring shuffle (i.e., it supports partial accumulation). 
***** mapValues 
def mapValues[U](f: V=>U): RDD[(K,U)]

***** countByKey
def countByKey():Map[K,Long]
Note that Map is a normal Scala Map and hence countByKey() is an action. 

***** Joins 
def join[W](other: RDD[(K,W)]): RDD[(K,(V,W))] //inner join 

//Note the use of Option to handle the case a key is missing in one RDD. 
def leftOuterJoin[W](other: RDD[(K,W)]): RDD[(K,(V,Option[W]))] 
def rightOuterJoin[W](other:RDD[(K,W}]): RDD[(K,(Option[V],W))] 

***** Q: can the tuple be three tuple? 
A: No, only one value seems to be recognized by /mapValues/. Solution group all values into a tuple. 
#+BEGIN_SRC  python
In [9]: rdd_a.collect()
Out[9]: [('a', 1, 2), ('a', 2, 3), ('b', 4, 5)]
In [10]: rv = rdd_a.mapValues(lambda x:x); rv.collect()
Out[10]: [('a', 1), ('a', 2), ('b', 4)]
#+END_SRC

***** Q: join vs. cogroup 
A: /RDD<K,V>.join(RDD<K,W>)/ return RDD<K,(V,W)> while /RDD<K,V>.cogroup(RDD<K,W>)/ return /RDD<K,(Seq[V],Seq[W])>. Also, in case of  duplicated keys, /join/ return a RDD with duplicated keys (similar as SQL handle join)  while the return of /cogroup/ has no duplicated key; There is also a function call groupByKey  which can use to handles RDD with duplicated keys. Under the hood, /join/ is implemented by /cogroup/. cogroup() can be used for much more than just implementing joins. We can also use it to implement intersect by key. Additionally, cogroup() can work on three or more RDDs at once. (Cf. Learning Spark: Chapter 4: Grouping Data). 
#+BEGIN_SRC python 
In [20]: rdd_a = sc.parallelize([('a',2), ('a',3),('b',4)])

In [21]: rdd_b = sc.parallelize([('a',5), ('a',6),('b',7)])

In [22]: rdd_a.cogroup(rdd_b).collect()
Out[22]: 
[('a',
  (<pyspark.resultiterable.ResultIterable at 0x7fa2922172d0>,
   <pyspark.resultiterable.ResultIterable at 0x7fa292157850>)),
 ('b',
  (<pyspark.resultiterable.ResultIterable at 0x7fa2921578d0>,
   <pyspark.resultiterable.ResultIterable at 0x7fa292157910>))]

#convert the return value from cogroup to RDD 
In [24]: rdd_a.cogroup(rdd_b).mapValues(lambda x: (list(x[0]),list(x[1]))).collect()
Out[24]: [('a', ([2, 3], [5, 6])), ('b', ([4], [7]))]        

In [27]: rdd_a.groupByKey().mapValues(lambda x: list(x)).collect()
Out[27]: [('a', [2, 3]), ('b', [4])]

# join 
In [28]: rdd_a.join(rdd_b).collect()
Out[28]: [('a', (2, 5)), ('a', (2, 6)), ('a', (3, 5)), ('a', (3, 6)), ('b', (4, 7))]

#+END_SRC 

***** Gotcha: 
+ Q: /reduce/ fails when applied to key-values pair? 
A: /reduce/ needs to return a key-values pair to continue the chain. For example, in the /word-count/, if we want to just count the total number of words from a RDD[(word,n)], we don't care the first one entry of the tuple but we still need to return a tuple in the reduce function. 
#+BEGIN_SRC python 
rdd = sc.textFile("r8_no_stop.txt")
input = rdd
tokenized = input.map(lambda l: l.split("\t")).filter(lambda l: l.size>0)
tokenized.map(lambda l: (l[0],1)).reduceByKey(lambda a,b: a+b).reduce(lambda x,y: x[1]+y[1]) #error: must return a tuple  
tokenized.map(lambda l: (l[0],1)).reduceByKey(lambda a,b: a+b).reduce(lambda x,y: ("x",x[1]+y[1]))

#+END_SRC

+ Q: /map/ fails when applied to PairRDD? 
A: Just like regular RDD, RDD[T].map a function with one parameter(which is a pair) instead of two parameters. 
#+BEGIN_SRC python
scala> val rdd = sc.parallelize(List(("a",1),("b",2), ("c",3)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[5] at parallelize at <console>:24

//What's wrong? 
scala> rdd.map((k:String, v: Int) => ("doh!",v))
<console>:27: error: type mismatch;
 found   : (String, Int) => (String, Int)
 required: ((String, Int)) => ?  //this tells us the function expect a function take one parameter(which is a pair) instead of two parameters. 
       rdd.map((k:String, v: Int) => ("doh!",v))

//the correct way:
rdd.map { case (k:String, v: Int) => ("doh!",v)}
                                  
#+END_SRC

val rdd = sc.parallelize(List(("a",1),("b",2), ("c",3)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[5] at parallelize at <console>:24

*** Partitioner 
Two kinds of partitioning available in Spark: Hash partitioning and Range partitioning. Both are only available for Key-value RDD, i.e, customer partitioning is only possible for key-value RDD.  
**** Customer Partitioner 
#+BEGIN_SRC scala 
val pairs = purchasesRDD.map(p => (p.customerId, p.price))//pairs now is a key-value RDD
val tunedPartitioner = new RangePartitioner(8, pairs) //note that RangePartitioner needs to sample the data so that it knows the range. 
val partitioned = pairs.partitionBy(tunedPartitioner).persist()//note the use of persist which avoids partitioning the data again and again. In fact, partitionBy should be always follow a persist.  


#+END_SRC

**** Q: What's the default partitioner? 
A: Depends on the number of executor and the number of cores of the executors. Cf. http://stackoverflow.com/questions/24877970/defaultminpartitions-in-textfile. The default number of partition depends on the the input format, for HDFS, it's 64MB per partition. There is a discussion for how many partition should be the default for textFile. https://github.com/mesos/spark/pull/718

**** Q: for binary operation(eg., /rdd_a.join(rdd_b)/), which partitionar is set on the output? 
A: If both /rdd_a/ and /rdd_b/ has partitioner set, it will be /rdd_a/'s partitioner; If only one of /rdd_a/ and /rdd_b/ has partitioner set, it will be that partitioner. Otherwise, it will be the a hash partitioner by default, with the number of partitions set to the level of parallelism of the operation(total number of cores in all executors). The rationale to try to stick to parent's partitioner is that the user can specify the partitioner for the rdd which the user know it would be expensive to shuffle.  

**** Q: what is the partitioner of an RDD which is generated by an transformation from another RDD. 
A: Some operation on PairRDD that hold to (and propagate) a partitioner: 
cogroup, groupWith, join, leftOuterJoin, rightOuterJoin, groupByKey and reduceByKey, foldByKey, combineByKey, partitionBy, sort, mapValues(if parent has a partitioner), flatMapValues(if parent has a partitioner), filter(if parent has a partitioner. 
Some operation loses parent's partitioner: map, flatMap which is because map, flatMap might change the key. 
There are also operations which automatically reset paritioner. For example, by default, when using sortByKey, a RangePartitioner is used. the default partitioner when using groupByKey is a HashPartitioner.

#+BEGIN_SRC scala
val purchasedPerCust = purchasesRdd.map(p=>(p.customerId,p.price)).groupByKey() #this create a hash-partitioned RDD. 

#+END_SRC

*** Shuffle
**** Q: How do I know whether a shuffle will happen
+ Look at the return type of certain transformation, eg., 
org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[366]

+ Use function toDebugString to see its execution plan: 
partitioned.reducedByKey((v1,v2) => (v1._1 + v2._1, v1._2 + v2._2).toDebugString 

#+BEGIN_SRC scala
scala> val wordCount = sc.textFile("README.md").flatMap(_.split("\\s+")).map((_, 1)).reduceByKey(_ + _)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24


// The (2) in the toDebugString indicates the level of parallelism in each stage, wordCount.getNumPartitions == 2  
scala> wordCount.toDebugString
res13: String =
(2) ShuffledRDD[21] at reduceByKey at <console>:24 []
 +-(2) MapPartitionsRDD[20] at map at <console>:24 []
    |  MapPartitionsRDD[19] at flatMap at <console>:24 []
    |  README.md MapPartitionsRDD[18] at textFile at <console>:24 []
    |  README.md HadoopRDD[17] at textFile at <console>:24 []
#+END_SRC


**** Q: When does "Shuffle" happen? 
A: Transformation may involve shuffling. Shuffle only happen when action occurs. 

**** Intermediate shuffling results are materialized. 
Recall that reconstruct the lost partition is fast for narrow dependencies but slow for wide dependencies. For wide dependencies(i.e, Shuffle), spark currently materialize intermediate records on the nodes holding parent partitions to simplify fault recovery, much like MapReduce materializes map outputs. (Cf. Spark White paper). The option /spark.shuffle.memoryFration/ controls the total amount of memory used in shuffle-related buffers. 

 
**** Narrow Dependencies and Wide Dependencies 
Not all transformations are created equal. Some transformation is more expensive than others. 
Narrow Dependency: each partition of the parent RDD is used by at most one partition of the child RDD(eg. map, filter, union, join with co-partitioned input). 
**** How do we know what other RDDs an RDD depends on? 
Use dependencies method 
#+BEGIN_SRC scala
val wordsRdd = sc.parallelize(largeList) 
val paris = wordsRdd.map(c => (c,1)).groupByKey().dependencies 

#+END_SRC

*** Persistence, Storage level(This is very unlike Scala Collections) 
Spark allows user to specify Where(memory or disk) and how to persist(serialized or unserialized) 
By default, each time we call a new action, the entire RDD must be computed "from scratch". To avoid this inefficiency, users can /persist/ immediate results.  Note that the use of /persist()/ in line a. Without it, the map in line a are evaluated each time an action is called(in this case, it's called numIterations time)  
#+BEGIN_SRC scala
val points = sc.textFile(...).map(parsePoint).persist() //a
val w = Vector.zeros(d)
for (i<- 1 to numIterations){
   val gradient = points.map { p=> (1/(1+exp(-p.y*w.dot(p.x)))-1) * p.y * p.y}.reduce(_+_)
w -= alpha * gradient 
}
#+END_SRC

Storage level: 
a). MEMORY_ONLY: in memory as regular Java objects
b). DISK_ONLY: on disk as regular Java objects
c). MEMORY_ONLY_SER: in memory as serialized Java objects(more compact)
d). MEMORY_AND_DISK 
e). MEMORY_AND_DISK_SER:Both in memory and on disk(spill over to disk to avoid re-computation). Note that a) doesn't spill to disk, it simply re-computes when the memory cannot hold the object anymore. 

Persist() vs. cache(): Persist takes a)- e) while cache() is a shorthand for default which is a). 
**** Serialized vs. Not-serialized?
Cache serialized objects instead of raw java object (use /MEMORY_ONLY_SER/,/MEMORY_AND_DISK_SER/). Caching serialized objects will slightly slow down the cache operation due to the cost of serialization. But it can substantially reduce the time spent on garbage collection in the JVM, since many individual records can be stored as a single giant serialized buffer. The is because the cost of GC scales with the number of objects on the heap, not the number of bytes of the data. We can find the  GC time in the application UI (driver-node:4040).




** Hadoop vs. Spark 
Hadoop relies on IO to connect iterations while Spark tries to do in-memory computation. 
** Programming Framework 
http://spark.apache.org/docs/latest/programming-guide.html
Cf. Learning Spark 
+ In Spark, there is no substantial benefit to write a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into small, more manageable operations (sgu: this refers Spark execution plan: multiple operations might be fused to one) 

*** Job, Stages, Tasks, 
/Job/: the set of stages for a particular action. 
/Stage/: In the simplest case, the scheduler outputs one stage for each RDD. In more complex cases, multiple RDDs are collapsed into a single stage(/pipeline/,/short-circuit/
/task/: A physical stage will launch tasks each do the same thing but on specific partitions of data.

*** Tunning and Debugging 
+ Cache serialized objects instead of raw java object (use /MEMORY_ONLY_SER/,/MEMORY_AND_DISK_SER/). Caching serialized objects will slightly slow down the cache operation due to the cost of serialization. But it can substantially reduce the time spent on garbage collection in the JVM, since many individual records can be stored as a single giant serialized buffer. The is because the cost of GC scales with the number of objects on the heap, not the number of bytes of the data. We can find the  GC time in the application UI (driver-node:4040). 

*** Session vs. Context
+ SparkContext represents a connection to a computing cluster, /pyspark/ has created a SparkContxt /sc/
+  SparkSession is introduced in Spark 2.0 and aims to provided simplified access and unified Functionality (like sql, create dataframe) https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html

** terminology 
Slice  = Partition 
** Text File: 
+ By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.
+ Q: How to know the record to have keys or not? 
TODO: read more about InputFormat in general

+ Q: Where does /rdd.saveAsTextFile(..)/ save the result? 
+ A: when HDFS exists, rdd.saveAsTextFile("/tmp/filename") saves to the HDFS system. In this case, the /filename/ is actually a directory under which there are the following files: _SUCCESS,  part-00000, part-00001, ect. If we ant to save the file to local file system, we should use /rdd.saveAsTextFile("file:///tmp/filename"). In this case, each executor save part of the results to their local file system, in executor 1, it's /tmp/filename/_temporary/task_201702192357_0024_m_00000/part-0000, in executor 2, it's /tmp/filename/_temporary/taks_201702192357_0024_m_00001/part-00001, ect. In the driver, it only has /tmp/filename/_SUCCESS without the _temporary and the part files. 

+ Q: what happen when /sc.textFile(...)/ read a file not in HDFS? 
A: In the presence of HDFS, if we use sc.textFile("/tmp/filename"), Spark will read /tmp/filename from HDFS. If we want to read from local file system, we should use sc.textFile("file:///tmp/filename"). In these case, /tmp/filename should exist in each executor. This is usaully the case when NFS is mounted at the same path on each node. If the file exist in each node(executor and driver) but different, the result seems to un-defined (sometime with EOF error, sometime succeed but the result is not the same as would be read local from each node). If all file present in all node and the same, /sc.textFile(...)/ seems always succeed and the record is not duplicated. 

 
#+END_SRC 

** Spark UI
+ Web UI: http://hostname:4040 on the machine where driver is running. One caveat is that in the case of YARN cluster mode, where the application driver runs inside the cluster, you should access the UI through YARN Resource manager, which proxies requests directly to the drive. 
+ Standalone cluster: http://hostname:8080
+ HDFS: http://hostname:50070
Note that spark-ec2 install two HDFS: ephemeral-hdfs and persistent-hdfs. By default, only ephemeral-hdfs is starting when we "launch" the cluster. 
** Monitoring and Tuning 
+ Use the Environment Tab in UI 
+ spark-submit --help 
+ http://spark.apache.org/docs/latest/configuration.html
+ Extra Java Option 
#+BEGIN_SRC shell
./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar
#+END_SRC
 
* Spark SQL
** Motivation 
Structure information enables optimization. Think about a query involves both filter and join. Operation order matters. Is it cool that Spark can automatically figure the right order. 
Three main goals of Spark SQL: 
1. Support relational processing both with Spark programs(on RDD) and on external data sources with friendly API
2. High performance 
3. Easily suport new data sources with semi-structured data and external databases. 
** Structured vs. Unstructured 
Structure: has a rigid schema, eg. SQL table. 
Semi-structure: has a schema but not as rigid, eg. json, xml 
unstructure: log files, images.
** DataFrame 
1. An relational API over RDD: for convenience. Note that Dataframe has its own API too. 
2. Able to be automatically optimized. 
3. Untyped: The elements within DataFrames are Row (Note that Row has no type parameter), which are not parameterized by a type. Therefore, the scala compiler cannot type check Spark SQL schemas in DataFrames.
*** DataFrame are RDD of Row("Untyped"). 

#+BEGIN_SRC  python 
In [2]: df = spark.createDataFrame([
   ...:     (0, "a b c".split(" ")),
   ...:     (1, "a b b c a".split(" "))
   ...: ], ["id", "words"])
In [16]: df.rdd.collect()
Out[16]: 
[Row(id=0, words=[u'a', u'b', u'c']),
 Row(id=1, words=[u'a', u'b', u'b', u'c', u'a'])]
#+END_SRC

Row are treated as generic ("untyped"). Scala compiler cannot type-check. 
#+BEGIN_SRC scala
df.filter("state" == "CA") #This complier even if no "state" column exist. 
#+END_SRC
  

*** Q: Why it's bad to be untyped?
A: In the following example, complier cannot catch the problem that the DataFrame df doesn't have state column.
#+BEGIN_SRC scala
df.filter("state" == "CA") 
#+END_SRC



*** Create DataFrame
    pyspark.sql.dataframe.DataFrame can be create directly from text, json, ect. Alternatively, it can also be created from a RDD. The schema can be inferred automatically or be supplied by StructType(which is a list of StructField). DataFrame.rdd returns a RDD for that dataframe(after spark 2.0). 
*** create DataFrame from RDD
#+BEGIN_SRC python 
    data_path = "r52-no-stop.txt" 
    spark = SparkSession\
        .builder\
        .appName("TfIdfExample")\
        .getOrCreate()

    #each line is a document; each document is represented by
    # a "word" for the class label followed by a TAB and then a seq of words
    # delimited by spaces
    sc = spark.sparkContext 
    documents = sc.textFile(data_path)\
                  .map(lambda l: l.split("\t"))\
                  .map(lambda p: Row(label=p[0],sentence=p[1]))

    documents = spark.createDataFrame(documents)
#+END_SRC

*** useful functions 
#+BEGIN_SRC python 
from pyspark.sql import SparkSession
from pyspark.sql import Row
spark = SparkSession.builder.appName("PythonSQL").config("spark.some.config.option","some-value").getOrCreate()
lines = sc.textFile("people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name = p[0], age= int(p[1])))
schemaPeople = spark.createDataFrame(people)
type(schemaPeople)  #Dataframe
schemaPeople.show(truncate=False)
schemaPeople.createOrReplaceTempView("people") #so that we can query the table through the SQL as follows. 
teenagers= spark.sql("SELECT name FROME people where age >=13 and age <=19")
teenagers= spark.sql("SELECT name FROM people where age >=13 and age <=19")
type(teenagers) #Dataframe
teenNames = teenagers.rdd.map(lambda: "Name: " + p.name)  #teenNames is a RDD
#+END_SRC

*** DataFrame API 
**** How to select columns? 
1. using $-notation(requires: import spark.implicits._): df.filter($'age' > 18)
2. Referring to the Dataframe: df.filter(df("age") > 18)
3. Using SQL query string: df.filter("age > 18")
#+BEGIN_SRC scala
case class Employee(id:Int, fname: String, lname: String, age: Int, city: String)
val employeeDF = sc.parallelize(...).toDF
val sydneyEmployeesDF = employeeEF.select("id","lname"). where("city=='Sydney'").orderBy("id"

// boolean
employeeDF.filter(($"age">25) && ($"city" == "Sydney")).show()
#+END_SRC 

**** GroupBy and Aggregation.  
The groupBy function which returns a RelationalGroupedDataset which usually is followed by an agg/max/count to get back a dataframe. 
#+BEGIN_SRC scala
//example 1 
case class Listing(street: String, zip: Int, price: Int) 
val listingsDF = .. //DataFrame of Listings. 
import org.apache.spark.sql.functions._
val mostExpensiveDF = listingsDF.groupBy($"zip").max("price")

//example 2
val postsDF = postsDF.groupBy($"authorID",$"subform")
                     .count($"authorID")
                     .orderBy($"subform",$"count(authorID)".desc)

#+END_SRC

**** Handling missing values in DataFrame.  
#+BEGIN_SRC scala 
// drop missing values: 
drop(), drop ("all") drop(Array("col1,"col2"))

//fill: 
fill(0)
fill(Map("minBalance" -> 0)) //fill only one column 
replace(Array("id"),Map(1 -> 2)) //replace 1 with 2 in Array column 

#+END_SRC

**** Joins on DataFrame 
#+BEGIN_SRC scala
df1.join(df2, $"df1.id" == $"df2.id","right_outer")
//or 
val df3 = df1.join(df2, df1("id") == df2("id"))

#+END_SRC

**** common functions 
#+BEGIN_SRC scala
collect(): Array[Row] 
count(): Long 
first(): Row/head(): Row  //head and first are the same
show(): Unit 
take(n:Int): Array[Row]
#+END_SRC

#+BEGIN_SRC python 
# number of rows, 
df.count() #return number of row regardless NA or not(vs. Pandas) 

# number of columns
len(df.columns)

# show the first row  
df.first()

#only take the first n row; 
#df.limit() return a new dataframe while head()/take() return a list of Row
df.limit(1).collect() [Row(age=2, name=u'Alice')]

# show unique values of a column 
df.select('col_name').distinct().show()

#split a dataset 
df_train, df_test = df.randomSplit([0.8,0.2])

#Stratified sampling: 
>>> from pyspark.sql.functions import col 
>>> dataset = sqlContext.range(0, 100).select((col("id") % 3).alias("key")) 
>>> sampled = dataset.sampleBy("key", fractions={0: 0.1, 1: 0.2}, seed=0) 
>>> sampled.groupBy("key").count().orderBy("key").show() 

#convert  row of vectors to np.array, /d/ below is a Row
w2v_plot = np.array(w2v.rdd.map(lambda d: d.vector.toArray()).take(100))

#+END_SRC
**** Optimization on DataFrame 
DataFrame API has built-in automatic optimization which RDD API cannot do. For example, for RDD API, the it's usually more efficient to first filter and then join; for DataFrame API, the operation order doesn't matter that much. Spark DataFrame use /Catalyst/ to optimize query. /Catalyst/ might re-order the operation, or reducing the amount of data requires IO(since it knows the schema). Spark DataFrame uses  /Tungsten/  off-heap(avoid garbage collection overhead) data encoder. Tungsten stores data in column-based.  
#+BEGIN_SRC scala 
df1.join(df2, df1("id") == df2("id")).filter().filter 

#+END_SRC

*** Schema 
*** print 
#+BEGIN_SRC python 
from pyspark.ml.feature import HashingTF, Tokenizer
training = spark.createDataFrame([
    (0L, "a b c d e spark", 1.0),
    (1L, "b d", 0.0),
    (2L, "spark f g h", 1.0),
    (3L, "hadoop mapreduce", 0.0)], ["id", "text", "label"])
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
rv = tokenizer.fit(training)
rv2 = hashingTF.transform(rv)
In [92]: type(rv2)
Out[92]: pyspark.sql.dataframe.DataFrame
In [90]: rv2.printSchema()
root
 |-- id: long (nullable = true)
 |-- text: string (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- features: vector (nullable = true)

#+END_SRC
*** metadata: 
http://stackoverflow.com/questions/32628845/is-there-a-way-to-add-extra-metadata-for-spark-dataframes
columns in dataframe can have metadata 
#+BEGIN_SRC python
In [18]: df = spark.createDataFrame(
    ...:     [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c
    ...: ")],
    ...:     ["id", "category"])
    ...: 

In [19]: indexer = StringIndexer(inputCol="category", outputCol="categ
    ...: oryIndex")

In [20]: model = indexer.fit(df)
    ...: indexed = model.transform(df)
    ...: 

In [21]: indexed.show()
    ...: 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+

In [23]: indexed.schema.fields
Out[23]: 
[StructField(id,LongType,true),
 StructField(category,StringType,true),
 StructField(categoryIndex,DoubleType,true)]

In [26]: indexed.printSchema()
root
 |-- id: long (nullable = true)
 |-- category: string (nullable = true)
 |-- categoryIndex: double (nullable = true)


In [24]: [t.metadata for t in indexed.schema.fields]
Out[24]: 
[{},
 {},
 {u'ml_attr': {u'name': u'categoryIndex',
   u'type': u'nominal',
   u'vals': [u'a', u'c', u'b']}}]


#+END_SRC
*** Chain operation 
Note the use of the df.select(..).withColumn(..) and col(""). About the use of col("exp") function: not needed if exp is just a column name, but needed if more complicated expression (Cf. https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/sql/Column.html) 
#+BEGIN_SRC python 
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
sentenceDataFrame = spark.createDataFrame([
    (0, "Hi I heard about Spark"),
    (1, "I wish Java could use case classes"),
    (2, "Logistic,regression,models,are,neat")
], ["id", "sentence"])
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
regexTokenizer = RegexTokenizer(inputCol="sentence", outputCol="words", pattern="\\W")
# alternatively, pattern="\\w+", gaps(False)
countTokens = udf(lambda words: len(words), IntegerType())
tokenized = tokenizer.transform(sentenceDataFrame)
tokenized.select("sentence", "words")\
    .withColumn("tokens", countTokens(col("words"))).show(truncate=False)
regexTokenized = regexTokenizer.transform(sentenceDataFrame)
regexTokenized.select("sentence", "words") \
    .withColumn("tokens", countTokens(col("words"))).show(truncate=False)

#+END_SRC



***  Dataframe vs. Pandas dataframe
http://www.kdnuggets.com/2016/01/python-data-science-pandas-spark-dataframe-differences.html
Spark dataframe is /immutatble/ and is designed for lazy evaluation, distributed computation. 





** Dataset (not available in Python)
Dataset allows query optimization and type-safety.
+ Dataset can be thought of as typed distributed collections of data.+ Dataset API unifies the DataFrame and RDD APIs. Mix and match functional operation(/typed transformace/) and relational operation(aka. /untyped transfroms/) as shown below. However, Catalyst cannot look into functional operation and hence cannot optimized those operations. 
#+BEGIN_SRC scala
//listingsDS is Dataset[Listing]
listingsDS.groupByKey(l => l.zip)  //looks like groupByKey on RDDs!
          .agg(avg($"price").as[Double])  //looks like DataFrame operations. 
#+END_SRC
+ Dataset require structured/semi-structured data. Schemas and Encoders core part of Datasets. 
+ In scala, Dataframe is just a type alias of Dataset[Row]  
type DataFrame = Dataset[Row] 
+ Dataset has more type info than DataFrame and more optimization than regular RDD. Even though Dataset is not completely type-safe, it's pretty much type-safe. 

+ More succinct syntax. 
df.collect() returns an Array[Row]; To collectly parse and cast a Row object, it's error prone as shown below. 
#+BEGIN_SRC scala 
case class Listing(street: String, zip: Int, price: Int)
val listingsDF = 
import org.apache.spark.sql.functions._ 
val averagePricesDF = listingsDF.groupBy($"zip").avg("price") 
val averagePrices = averagePricesDF.collect()//Array[Row]


//You might think price is an Int, so you do 
val averagePricesAgain = averagePrices.map { 
row => (row(0).asInstanceOf[String], row(1).asInstanceOf[Int])
}//casting error: since the type is wrong. 
//Therefore, we usually need to look at the schema of Row 
averagePrices.head.schema.printTreeString()

/The casting works but error prone.. 
val averagePricesAgain = averagePrices.map {
row => (row(0).asInstanceOf[Int], row(1).asInstanceOf[Double])
}
#+END_SRC

*** create a Dataset 
#+BEGIN_SRC scala
//create a dataset from a dataframe. 
myDF.toDS //requires import spark.implicits._

//from RDD 
myRDD.toDS

//from json 
val myDS = spark.read.json("people.json").as[Person]

#+END_SRC

*** TypedColumn
Dataset operates on /org.apache.spark.sql.TypedColumn/ not  /org.apache.spark.sql.Column/ not  
#+BEGIN_SRC scala
.agg(avg($"price")).show //error: avg($"price") is Column not TypedColumn, use avg($"price").as[Double] instead. 
#+END_SRC

*** Aggregation for Dataset

Dataset has no reduceByKey function. We can emulate  the same reduceByKey semantics by using groupByKey, mapGroup, mapValues and reduceGroups as shown below. 

**** The Defintions 
Note the def of Dataset.groupByKey, KeyValueGroupedDataset.mapValues 
#+BEGIN_SRC scala
class Dataset[T] extends Serializable
def groupByKey[K](func: (T) ⇒ K)(implicit arg0: Encoder[K]): KeyValueGroupedDataset[K, T]

class KeyValueGroupedDataset[K,V] extends Serializable 
def mapGroups[U](f: (K, Iterator[V]) ⇒ U)(implicit arg0: Encoder[U]): Dataset[U]
def mapValues[W](func: (V) ⇒ W)(implicit arg0: Encoder[W]): KeyValueGroupedDataset[K, W]
def reduceGroups(f: (V, V) ⇒ V): Dataset[(K, V)]
#+END_SRC

****  Dataset[T].groupByKey 
Note that /Dataset[T].groupByKey/ return /KeyValueGroupedDataset[K,V]/ which is NOT a Dataset. This is unlike the RDD[(K,V)].groupByKey which returns another RDD[(K, Iterable[V])]
#+BEGIN_SRC scala
class Dataset[T] extends Serializable
def groupByKey[K](func: (T) ⇒ K)(implicit arg0: Encoder[K]): KeyValueGroupedDataset[K, T]
#+END_SRC

**** KeyValueGroupedDataset.mapGroups
This works but not efficient since mapGroup *doesn't support partial aggregation* and as a result shuffling all the data in the dataset. /mapValues/  and /reduceGroups/ are more efficient. 
#+BEGIN_SRC scala
//task: concat the string with the same key 
val keyValues = List((3,"Me"), (1,"Thi"), (2,"Se"), (3, "ssa"), (1, "sIsA"), (3, "ge:"), (3, "-)"), (2, "cre"), (2, "t"))
val keyValuesDS = keyValues.toDS //use Scala implicit conversion, require import spark.implicits._ 

keyValuesDS.groupByKey(p=>p._1)
           .mapGroups((k,vs) => (k, vs.foldLeft("")((acc,p) => acc + p._2))) # note that vs is an Iterator[(Int,String)] 
           .sort("_1").show() //sort by the key again. 

#+END_SRC 

**** /mapValues/ and /reduceGroups/ 
#+BEGIN_SRC scala
/task: concat the string with the same key 
val keyValues = List((3,"Me"), (1,"Thi"), (2,"Se"), (3, "ssa"), (1, "sIsA"), (3, "ge:"), (3, "-)"), (2, "cre"), (2, "t")) 
val keyValuesDS = keyValues.toDS //use Scala implicit conversion, require import spark.implicits._ 

keyValuesDS.groupByKey(p=> p._1) //return KeyValueGroupedDataset[Int, (Int, String)]
           .mapValues( p => p._2) // 
           .reduceGroups((acc,str) => acc+ str)
#+END_SRC

****  Aggregators. 
#+BEGIN_SRC scala
//definition, require import org.apache.spark.sql.expressions
abstract class Aggregator[-IN, BUF, OUT] extends Serializable
abstract def bufferEncoder: Encoder[BUF]
abstract def finish(reduction: BUF): OUT
abstract def merge(b1: BUF, b2: BUF): BUF
abstract def outputEncoder: Encoder[OUT]
abstract def reduce(b: BUF, a: IN): BUF
abstract def zero: BUF
def toColumn: TypedColumn[IN, OUT]
#+END_SRC
 
#+BEGIN_SRC scala 
//task: concat the string with the same key 
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.Encoders
val keyValues = List((3,"Me"), (1,"Thi"), (2,"Se"), (3, "ssa"), (1, "sIsA"), (3, "ge:"), (3, "-)"), (2, "cre"), (2, "t"))
val keyValuesDS = keyValues.toDS //use Scala implicit conversion, require import spark.implicits._ 

val strConcat = new Aggregator[(Int,String), String, String]{
def zero: String = ""
def reduce(b:String, a: (Int, String)):String = b + a._2
def merge(b1:String, b2:String): String = b1+ b2
def finish(r:String): String =r 

//? Why we need override? Actually, it's not needed. 
override def bufferEncoder: Encoder[String] = Encoders.STRING #Note tat Encoders is an object while Encoder is a trait with a type parameter
override def outputEncoder: Encoder[String] = Encoders.STRING
}.toColumn 

keyValuesDS.groupByKey(pair => pair._1).agg(strConcat).show

#+END_SRC


** RDD vs. Dataset vs. DataFrame
http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes

Q: When to use which? (From Coursera Spark Course)
Use RDD if 
+ you have unstructured data 
+ You need to super fine-tune and manage low-level details of RDD computations. You cannot trust the optimizer to do it for you 
+ You have complex data types and cannot be serialized with Encoders. Encoders cannot yet nicely deal with user-define type. 

Use Dataset if 
+ You have structured/semi-structured. You want to use the read function from json and then map to a case class. 
+ You want to work with functional API 
+ You need good performance, but it doesn't have to be the best. 

Use DataFrames when 
+ You have structured/semi-structured data 
+ You want to rely on as much as possible the optimizer 
+ You can deal with the fact it's untyped. 




** Q: What literals(statements) are available in Spark SQL. 
A: Largely what's available in HiveSQL.  


* Spark MLlib
** Pipeline
*** Summary
+ inspired by /sklearn/, and relied on Dataframe.  
+ Transformers, Estimators, Evaluator
Transformer.transform()s and Estimator.fit()s are both stateless. /Model/ is a transformer.  
+ Pipeline can be nonlinear (DAG) 
+ Same instance cannot be inserted into the pipeline twice(but can be two instance of the same type)
+ PipelineModel consists of multiple transformers and estimators(Cf. Pipeline.scala) 
#+BEGIN_SRC python
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

# Prepare training documents from a list of (id, text, label) tuples.
training = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0)
], ["id", "text", "label"])

# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# Fit the pipeline to training documents.
model = pipeline.fit(training)

# Prepare test documents, which are unlabeled (id, text) tuples.
test = spark.createDataFrame([
    (4, "spark i j k"),
    (5, "l m n"),
    (6, "spark hadoop spark"),
    (7, "apache hadoop")
], ["id", "text"])

# Make predictions on test documents and print columns of interest.
prediction = model.transform(test)
selected = prediction.select("id", "text", "probability", "prediction")
for row in selected.collect():
    rid, text, prob, prediction = row
    print("(%d, %s) --> prob=%s, prediction=%f" % (rid, text, str(prob), prediction))
#+END_SRC
*** vs. Pipeline in sklearn 
sklearn Pipeline consists of both transformers and estimators. Transformers in sklearn implement both /fit/ and /transfrom/ method while estimators in sklearn implements /fit/ and /predict/ methods. Also, transformers/estimators in sklearn might has states while /transformers/ and /estimators/ in Spark are *stateless*. /estimators/ in sklearn rely on its internal state for prediction while /estimators/ in spark returns a /model/(which is a transformer)
Cf. [[https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/pipeline.py#L86][Pipeline:_validate_steps]]
Cf. [[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html][FunctionTransformer]]
Cf. [[http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator][rolling your own estimator]]

*** Param and ParamMap
+ MLlib /Estimators/ and /Transformer/s use a uniform API for specifying parameters.
+ Parameters belong to specific instances of Estimators and Transformers. For example, if we have two LogisticRegression instances lr1 and lr2, then we can build a ParamMap with both maxIter parameters specified: ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20).
#+BEGIN_SRC 
# to see the parameters for an estimator 
In [33]: lr = LogisticRegression(maxIter=10, regParam=0.01)
In [34]: print("LogisticRegression parameters:\n" + lr.explainParams()
    ...:  + "\n")
In [39]: lr.maxIter
Out[39]: Param(parent=u'LogisticRegression_4f8190d0025ad21226c0', name='maxIter', doc='max number of iterations (>= 0).')

n [41]: paramMap = {lr.maxIter: 20}
In [42]: paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) 
In [43]: paramMap2 = {lr.probabilityCol: "myProbability"}
In [44]: paramMapCombined = paramMap.copy()
In [45]: paramMapCombined.update(paramMap2)
In [46]: model2 = lr.fit(training, paramMapCombined)

# extractParamMap() is not working in PySpark https://issues.apache.org/jira/browse/SPARK-13368
print(model2.extractParamMap()) 
#+END_SRC

*** Cross Validation 
After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset.
Q: how to extract the best model?
A: /cvModel.bestModel/; if /cvModel/ is a Pipeline model, each stage can be retrieved by /cvModel.bestMode.stages[i]/ 
Q: how to extract the best parameters from cvModel
A: look at: /cvModel.getEstimatorParamMaps()/ and /cvModel.avgMetrics/ 
#+BEGIN_SRC python 
#print the metrics for each combination of parameters in cv search 
paramNames_pp=[";".join(["%s:%g" % (k.name,v) for k,v in p.items()]) for p in cvModel.getEstimatorParamMaps()]
print(zip(paramNames_pp, cvModel.avgMetrics))
#+END_SRC

** Vector and Array 
#+BEGIN_SRC python 
In [94]: v1
Out[94]: DenseVector([0.0, 1.1, 0.1])
In [95]: v2 = 
KeyboardInterrupt
In [95]: v2 = Vectors.dense([0.0, 1.1, -.1])
In [96]: rv = sc.parallelize([v1,v2])
In [97]: rv
Out[97]: ParallelCollectionRDD[149] at parallelize at PythonRDD.scala:475
#+END_SRC
*** Sparse Vector and Sparse matrix 
#+BEGIN_SRC python 
from pyspark.mllib.linalg import Vectors
import scipy.sparse as sps 
sv1 = Vectors.sparse(3,[0,2],[1.0,3.0]) # the first integer is the size
sv2 = sps.csc_matrix(..)
#+END_SRC
SVD 
#+BEGIN_SRC scala
//From the book of Learning Spark 
val svd = mat.computeSVD(3,computeU=true) //the type of svd is SingularValueDecomposition[RowMatrix,Matrix] 
val s: Vector = svd.s
val V: Matrix = svd.V #V is a local matrix.for the first k svd components, V is of size n by k 
#+END_SRC

*** Matrix multiplcation 
Rf. /Learning Spark/
+ Local vector/matrix depends on the API. for Python, we can usually directly pass numpy array or use the /pyspark.mllib.linalg.Vectors/ which is a wrapper round numpy. In particular, we can get the numpy array from pyspark array using the /toArray/ method ; for Scala/Java, it's /org.apache.spark.mllib.linalg.Vectors/ which are wrappers around /Breeze/ array. spark mllib around are primarily meant for data representation and do not provide arithmetic operations such as addition and subtraction. 
+ Distributed matrices are wrappers around RDD[]. 
+ Note that the indices of /IndexedRowMatrix/ might not be consecutive(http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix). When a /IndexedRowMatrix/ multiplies a local matrix, it converts to /RowMatrix/ and then zip back the indices. 
#+BEGIN_SRC scala
scala> val idx = "^v([0-9]+)$".r
idx: scala.util.matching.Regex = ^v([0-9]+)$
scala> val rdd = sc.parallelize(List(
      (("v1","v1"),2),(("v1","v2"),4),  
      (("v1","v3"),1),(("v2","v2"),5),  
      (("v2","v3"),1),(("v3","v3"),2)   
      ))                                
scala> val mat = new CoordinateMatrix(rdd.map {case ((idx(i),idx(j)),v) =>
      MatrixEntry(i.toLong-1,j.toLong-1,v.toDouble) 
      }).toIndexedRowMatrix                         
                                                  

//we can multiply a distributed matrix by a local matrix(only in Scala API, not Python)
val vector = Matrices.dense(3,1,Array(4.0,1.0,5.0))
mat.multiply(vector).rows.map(println).collect()

//or we can do it in map-reduce way 
val vec = sc.parallelize(List(("v1",4),("v2",1),("v3",5)))
val t = (rdd.map {case ((i,j),v) => (j,(i,v))} join vec).values map {case ((i,v1),v2) => (i,v1*v2)} reduceByKey(_+_)

//If we want to multiply two distributed matrices, both of them needs to be block matrix. 


#+END_SRC
** Feature Engineering 
*** Tokenizer 
*** TF: 
+ Hashing TF:
Why number of buckets is advisable to be 2^k?
To read: https://en.wikipedia.org/wiki/Feature_hashing
+ ConutVectorizer:
/CountVectorizer/ is an /estimator/. During the fitting process, CountVectorizer will select the top /vocabSize/ words ordered by term frequency across the corpus. CountVectorizer.fit() generates a /CountVectorizerModel/. /CountVectorizerModel::transform(df_test)/ counts the term frequency in /df_test/ according to the /vocabulary/; the words not in the /vocabulary/ are ignored.  
#+BEGIN_SRC python
In [2]: df = spark.createDataFrame([
   ...:     (0, "a b c".split(" ")),
   ...:     (1, "a b b c a".split(" "))
   ...: ], ["id", "words"])

In [3]: cv = CountVectorizer(inputCol="words", outputCol="features", vocabSize=3, minDF=2.0)

In [4]: model = cv.fit(df)

In [5]: model.vocabulary
Out[5]: [u'b', u'a', u'c']

In [9]: df_test = spark.createDataFrame([(0, "c c c d a".split(" "))],["id","words"])

In [10]: df_test.show()
+---+---------------+
| id|          words|
+---+---------------+
|  0|[c, c, c, d, a]|
+---+---------------+


In [11]: model.transform(df)
Out[11]: DataFrame[id: bigint, words: array<string>, features: vector]

In [12]: model.transform(df).show(truncate=True)
+---+---------------+--------------------+
| id|          words|            features|
+---+---------------+--------------------+
|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|
|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|
+---+---------------+--------------------+


In [13]: model.transform(df).show(truncate=False)
+---+---------------+-------------------------+
|id |words          |features                 |
+---+---------------+-------------------------+
|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|
|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|
+---+---------------+-------------------------+


In [14]: model.transform(df_test).show(truncate=False)
+---+---------------+-------------------+
|id |words          |features           |
+---+---------------+-------------------+
|0  |[c, c, c, d, a]|(3,[1,2],[1.0,3.0])|
+---+---------------+-------------------+

#+END_SRC
*** IDF: 
Note that IDF doesn't normal the vector(vs. TfidfVectorizer in sklearn)
*** Word2Vec
The tutorial is misleading regarding to this example. sample_lda_data.txt is the training data for this example, instead of text8 data(even though this is the standard training set for word2vec). http://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/feature.html#Word2Vec
#+BEGIN_SRC python
from pyspark.mllib.feature import Word2Vec

#sgu: 
inp = sc.textFile("data/mllib/sample_lda_data.txt").map(lambda row: row.split(" "))

word2vec = Word2Vec()
model = word2vec.fit(inp)

synonyms = model.findSynonyms('1', 5)

for word, cosine_distance in synonyms:
    print("{}: {}".format(word, cosine_distance))
#+END_SRC
*** StringIndexer, Index2String  
+ StringIndexer can be used to turn string into numeric index. As Spark 2.1.0, classifiers cannot handle string label so StringIndexer is needed  
#+BEGIN_SRC python 
In [13]: df = spark.createDataFrame(
    ...:     [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c
    ...: ")],
    ...:     ["id", "category"])
    ...: 

In [14]: indexer = StringIndexer(inputCol="category", outputCol="categ
    ...: oryIndex")

In [15]: indexed = indexer.fit(df).transform(df)

In [16]: indexed.show()
    ...: 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+


In [17]: indexed.printSchema()
root
 |-- id: long (nullable = true)
 |-- category: string (nullable = true)
 |-- categoryIndex: double (nullable = true)

#+END_SRC

+ Q: how to map the index back to string? 
We can use the meta data in the column created by StringIndexer to convert numeric column back to string. 
#+BEGIN_SRC python 
    #Map back the predict index to label name if needed 
    df, df_pred = documents, df_train 
    name_mapping = dict([(col,t.metadata) for col, t in zip(df.columns,df.schema.fields)])['label_n']['ml_attr']['vals']
    df_pred = IndexToString(inputCol='label_pred_n',outputCol='label_pred',labels = name_mapping).transform(df_pred)
#+END_SRC

** text segmentation
Stanford NLP Group, scalanlp/chalk 
*** Word2Vec
Estimator also. 
http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec
** Classification
*** What's the rawPrediction as in LogisticRegression ctor?
For LogisticRegression, it's the logit (the linear function value, equiv. to  /prediction(glm_fit,new_data type="link") in R/ 
http://stackoverflow.com/questions/37903288/what-do-colum-rawprediction-and-probability-of-dataframe-mean-in-spark-mllib
*** What's aggregationDepth param in LogisticRegression ctor? 
This is related to an improvement of MLlib regarding to MapReduce Paradigm. Multi-level aggregation tree is used to combined partially on a small set of executors before they sent to the drivers which is dramatically reduces the load of the driver. https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html
Note this WIP: https://issues.apache.org/jira/browse/SPARK-17090

* Setting up Spark-EC2

** dissect spark_ec2.py
./spark-ec2 -k sgu-ec2-all -i $HOME/.ssh/sgu-ec2-all.pem -s 2 launch spark_test  
-s: the number of slave 
'launch': the action 
'spark_test': cluster_name 
/cluster_name/ is used to identify the security group.   
*** launch_cluster
1. get or create security group /cluster_name/-master , /cluster_name/-slaves
2. authorize the security group (handle vpc)  so that master can access slaves
3. check whether the cluster has already exist /get_exsiting_cluster/
4. get with AMI with Spark 
   which looks up git-repos based on region and instance type: eg. 'https://raw.github.com/amplab/spark-ec2/branch-2.0/ami-list/us-east-1/pvm'
5. create EBS device mapping so that we can add EBS volumne if asked to (vol.size, vol.type, attached_name(/dev/sds,/dev/sdt,ect.), delete_on_termination).
6. launch slave instance or request spot instances (the instances is collected in /slave_nodes/
7. launch master instance(only one, not a spot instance)
8. tag the master and slaves 
9. return /master_nodes/ and /slave_nodes/
*** get_existing_cluster
return /master_instance/, /slave_instances/
find /cluster_name/+'-master' and /cluster_name/+'-slaves'
*** wait_for_cluster_state
make sure all master_nodes, and slave_instances are ssh-ready. 

*** setup_cluster
Q: What is swap space (specified by input argument =-swap=)?
Q: what is tachyon?   
1. on master: create a ssh key and copy to the slaves. 
2. clone spark-ec2 git-repos 
3. prepare a local file with all necessary parameters filled  and then copy the file (rsync) to master. 
4.  deploy other user files (parameter /deploy_root_dir/) to master
5. run /setup_spark_cluster/, which run spark-ec2/setup.sh which fills in the package config files under /spark-ec2/templates/ and write the conf to each individual package. Note eventually after a package is setup, part of the package is from other site(eg. S3) and part of it is from the /templates/ directory.  
 



** A simple Version
#+BEGIN_SRC python 
import boto
from boto.ec2.blockdevicemapping import BlockDeviceMapping, BlockDeviceType, EBSBlockDeviceType
from boto import ec2

conn = ec2.connect_to_region("us-east-1")

#for region: us-east-1, instance type: pvm;
# given in 'https://raw.github.com/amplab/spark-ec2/branch-2.0/ami-list/us-east-1/pvm'
# Amazon Linux, username: root. 
ami_name = 'ami-5bb18832'
image = conn.get_image(ami_name)

#launch only one instance 
#there are more kwargs to launch multiple intance, attached EBS, run a configure script, ect. 
#need to specify aws_access_key_id and aws_secret_access_key in ~/.boto or in evironment
# the private pem key associated with the key_name is needed to ssh into the instance later.
# return a reservation
res = image.run(key_name = "sgu-ec2-all",
                instance_type = "m1.large",
                security_groups=["launch-wizard-1"])

time.sleep(20)   #wait for 20 secs 
res = [s for s in conn.get_all_reservations() if s.id == res.id][0]
print "ssh -i path-to-pem root@%s" % (res.instances[0].ip_address,)

#+END_SRC

