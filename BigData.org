* Spark

** RDD: 
+ Action vs. Transformation 
+ Persistence, Storage level: 
Where(memory or disk) and how to persist(serialized or unserialized) 
each time we call a new action, the entire RDD must be computed "from scratch". To avoid this inefficiency, users can /persist/ immediate results. 
*** Transformations
Elementwise and pseduo-set 

*** Actions
+ /reduce()/, /fold()/
  return type has to be the element type of RDD
+ /aggregate()/
  can be any return type; caller needs to provides two functions, one for reduction, one for merging two accumulators, given that each node accumulates its own results locally 

+ Use /foreach/ if no value needs to return. 

Q: What's the difference between /fold()/ vs. /reduce()/
Practical difference between these methods in Spark is only related to their behavior on empty collections and ability to use mutable buffer (arguably it is related to performance)
http://stackoverflow.com/questions/36056895/reduce-vs-fold-in-apache-spark
http://stackoverflow.com/questions/34529953/why-is-the-fold-action-necessary-in-spark

 
*** Specialized RDD: DoubleRDDFunctions, PairRDDFunction
+ More functions are available for JavaDoubleRDD. We can use /rdd.mapToDouble(..)/ to get a JavaDoubleRDD so that /mean()/,/variance()/ is available.  
+ In Scala, Implicit conversion happens to convert RDD<Double> to DoubleRDD.  

Gotcha: 
+ Q: /reduce/ fails when applied to key-values pair? 
A: /reduce/ needs to return a key-values pair to continue the chain. For example, in the /word-count/, if we want to just count the total number of words from a RDD[(word,n)], we don't care the first one entry of the tuple but we still need to return a tuple in the reduce function. 
#+BEGIN_SRC python 
rdd = sc.textFile("r8_no_stop.txt")
input = rdd
tokenized = input.map(lambda l: l.split("\t")).filter(lambda l: l.size>0)
tokenized.map(lambda l: (l[0],1)).reduceByKey(lambda a,b: a+b).reduce(lambda x,y: x[1]+y[1]) #error: must return a tuple  
tokenized.map(lambda l: (l[0],1)).reduceByKey(lambda a,b: a+b).reduce(lambda x,y: ("x",x[1]+y[1]))

#+END_SRC
 
**** PairRDD
***** Q: can the tuple be three tuple? 
A: No, only one value seems to be recognized by /mapValues/
#+BEGIN_SRC  python
In [9]: rdd_a.collect()
Out[9]: [('a', 1, 2), ('a', 2, 3), ('b', 4, 5)]
In [10]: rv = rdd_a.mapValues(lambda x:x); rv.collect()
Out[10]: [('a', 1), ('a', 2), ('b', 4)]
#+END_SRC

***** Q: join vs. cogroup 
A: /RDD<K,V>.join(RDD<K,W>)/ return RDD<K,(V,W)> while /RDD<K,V>.cogroup(RDD<K,W>)/ return /RDD<K,(Seq[V],Seq[W])>. Also, in case of  duplicated keys, /join/ return a RDD with duplicated keys (similar as SQL handle join)  while the return of /cogroup/ has no duplicated key; There is also a function call groupByKey  which can use to handles RDD with duplicated keys. 
#+BEGIN_SRC python 
In [20]: rdd_a = sc.parallelize([('a',2), ('a',3),('b',4)])

In [21]: rdd_b = sc.parallelize([('a',5), ('a',6),('b',7)])

In [22]: rdd_a.cogroup(rdd_b).collect()
Out[22]: 
[('a',
  (<pyspark.resultiterable.ResultIterable at 0x7fa2922172d0>,
   <pyspark.resultiterable.ResultIterable at 0x7fa292157850>)),
 ('b',
  (<pyspark.resultiterable.ResultIterable at 0x7fa2921578d0>,
   <pyspark.resultiterable.ResultIterable at 0x7fa292157910>))]

#convert the return value from cogroup to RDD 
In [24]: rdd_a.cogroup(rdd_b).mapValues(lambda x: (list(x[0]),list(x[1]))).collect()
Out[24]: [('a', ([2, 3], [5, 6])), ('b', ([4], [7]))]        

In [27]: rdd_a.groupByKey().mapValues(lambda x: list(x)).collect()
Out[27]: [('a', [2, 3]), ('b', [4])]

# join 
In [28]: rdd_a.join(rdd_b).collect()
Out[28]: [('a', (2, 5)), ('a', (2, 6)), ('a', (3, 5)), ('a', (3, 6)), ('b', (4, 7))]

#+END_SRC 


*** Partitioner 
**** Q: What's the default partitioner? 
A: Depends on the number of executor and the number of cores of the executors. Cf. http://stackoverflow.com/questions/24877970/defaultminpartitions-in-textfile. The default number of partition depends on the the input format, for HDFS, it's 64MB per partition. There is a discussion for how many partition should be the default for textFile. https://github.com/mesos/spark/pull/718

**** Q: for binary operation(eg., /rdd_a.join(rdd_b)/), which partitionar is set on the output? 
A: If both /rdd_a/ and /rdd_b/ has partitioner set, it will be /rdd_a/'s partitioner; If only one of /rdd_a/ and /rdd_b/ has partitioner set, it will be that partitioner. Otherwise, it will be the a hash partitioner by default, with the number of partitions set to the level of parallelism of the operation. 
*** Shuffle

+ Q: When does "Shuffle" happen? 
A: Transformation may involve shuffling. Shuffle only happen when action occurs. 

+ For wide dependencies(i.e, Shuffle), spark currently materialize intermediate records on the nodes holding parent partitions to simplify fault recovery, much like MapReduce materializes map outputs. (Cf. Spark White paper). The option /spark.shuffle.memoryFration/ controls the total amount of memory used in shuffle-related buffers. 

 


** Programming Framework 
http://spark.apache.org/docs/latest/programming-guide.html
Cf. Learning Spark 
+ In Spark, there is no substantial benefit to write a single complex map instead of chaining together many simple operations. Thus, users are free to organize their program into small, more manageable operations. 

*** Job, Stages, Tasks, 
/Job/: the set of stages for a particular action. 
/Stage/: In the simplest case, the scheduler outputs one stage for each RDD. In more complex cases, multiple RDDs are collapsed into a single stage(/pipeline/,/short-circuit/
/task/: A physical stage will launch tasks each do the same thing but on specific partitions of data.

*** Tunning and Debugging 
+ Cache serialized objects instead of raw java object (use /MEMORY_ONLY_SER/,/MEMORY_AND_DISK_SER/). Caching serialized objects will slightly slow down the cache operation due to the cost of serialization. But it can substantially reduce the time spent on garbage collection in the JVM, since many individual records can be stored as a single giant serialized buffer. The is because the cost of GC scales with the number of objects on the heap, not the number of bytes of the data. We can find the  GC time in the application UI (driver-node:4040). 

*** Session vs. Context
+ SparkContext represents a connection to a computing cluster, /pyspark/ has created a SparkContxt /sc/
+  SparkSession is introduced in Spark 2.0 and aims to provided simplified access and unified Functionality (like sql, create dataframe) https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html

** terminology 
Slice  = Partition 
** Text File: 
+ By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.
+ Q: How to know the record to have keys or not? 
TODO: read more about InputFormat in general

+ Q: Where does /rdd.saveAsTextFile(..)/ save the result? 
+ A: when HDFS exists, rdd.saveAsTextFile("/tmp/filename") saves to the HDFS system. In this case, the /filename/ is actually a directory under which there are the following files: _SUCCESS,  part-00000, part-00001, ect. If we ant to save the file to local file system, we should use /rdd.saveAsTextFile("file:///tmp/filename"). In this case, each executor save part of the results to their local file system, in executor 1, it's /tmp/filename/_temporary/task_201702192357_0024_m_00000/part-0000, in executor 2, it's /tmp/filename/_temporary/taks_201702192357_0024_m_00001/part-00001, ect. In the driver, it only has /tmp/filename/_SUCCESS without the _temporary and the part files. 

+ Q: what happen when /sc.textFile(...)/ read a file not in HDFS? 
A: In the presence of HDFS, if we use sc.textFile("/tmp/filename"), Spark will read /tmp/filename from HDFS. If we want to read from local file system, we should use sc.textFile("file:///tmp/filename"). In these case, /tmp/filename should exist in each executor. This is usaully the case when NFS is mounted at the same path on each node. If the file exist in each node(executor and driver) but different, the result seems to un-defined (sometime with EOF error, sometime succeed but the result is not the same as would be read local from each node). If all file present in all node and the same, /sc.textFile(...)/ seems always succeed and the record is not duplicated. 

 
#+END_SRC 

** Spark UI
+ Web UI: http://hostname:4040 on the machine where driver is running. One caveat is that in the case of YARN cluster mode, where the application driver runs inside the cluster, you should access the UI through YARN Resource manager, which proxies requests directly to the drive. 
+ Standalone cluster: http://hostname:8080
+ HDFS: http://hostname:50070
Note that spark-ec2 install two HDFS: ephemeral-hdfs and persistent-hdfs. By default, only ephemeral-hdfs is starting when we "launch" the cluster. 
** Monitoring and Tuning 
+ Use the Environment Tab in UI 
+ spark-submit --help 
+ http://spark.apache.org/docs/latest/configuration.html
+ Extra Java Option 
#+BEGIN_SRC shell
./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar
#+END_SRC
 
* Spark SQL
** RDD vs. Dataset vs. DataFrame
http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes
DataFrame are RDD of Row 
#+BEGIN_SRC  python 
In [2]: df = spark.createDataFrame([
   ...:     (0, "a b c".split(" ")),
   ...:     (1, "a b b c a".split(" "))
   ...: ], ["id", "words"])
In [16]: df.rdd.collect()
Out[16]: 
[Row(id=0, words=[u'a', u'b', u'c']),
 Row(id=1, words=[u'a', u'b', u'b', u'c', u'a'])]
#+END_SRC

**  Dataframe vs. Pandas dataframe
http://www.kdnuggets.com/2016/01/python-data-science-pandas-spark-dataframe-differences.html
Spark dataframe is /immutatble/ and is designed for lazy evaluation, distributed computation. 

** Create DataFrame
pyspark.sql.dataframe.DataFrame can be create directly from text, json, ect. Alternatively, it can also be created from a RDD. The schema can be inferred automatically or be supplied by StructType(which is a list of StructField). DataFrame.rdd returns a RDD for that dataframe(after spark 2.0). 
*** create DataFrame from RDD
#+BEGIN_SRC python 
    data_path = "r52-no-stop.txt" 
    spark = SparkSession\
        .builder\
        .appName("TfIdfExample")\
        .getOrCreate()

    #each line is a document; each document is represented by
    # a "word" for the class label followed by a TAB and then a seq of words
    # delimited by spaces
    sc = spark.sparkContext 
    documents = sc.textFile(data_path)\
                  .map(lambda l: l.split("\t"))\
                  .map(lambda p: Row(label=p[0],sentence=p[1]))

    documents = spark.createDataFrame(documents)
#+END_SRC

*** useful functions 
#+BEGIN_SRC python 
from pyspark.sql import SparkSession
from pyspark.sql import Row
spark = SparkSession.builder.appName("PythonSQL").config("spark.some.config.option","some-value").getOrCreate()
lines = sc.textFile("people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name = p[0], age= int(p[1])))
schemaPeople = spark.createDataFrame(people)
type(schemaPeople)  #Dataframe
schemaPeople.show(truncate=False)
schemaPeople.createOrReplaceTempView("people") #so that we can query the table through the SQL as follows. 
teenagers= spark.sql("SELECT name FROME people where age >=13 and age <=19")
teenagers= spark.sql("SELECT name FROM people where age >=13 and age <=19")
type(teenagers) #Dataframe
teenNames = teenagers.rdd.map(lambda: "Name: " + p.name)  #teenNames is a RDD

** Schema 
*** print 
#+BEGIN_SRC python 
from pyspark.ml.feature import HashingTF, Tokenizer
training = spark.createDataFrame([
    (0L, "a b c d e spark", 1.0),
    (1L, "b d", 0.0),
    (2L, "spark f g h", 1.0),
    (3L, "hadoop mapreduce", 0.0)], ["id", "text", "label"])
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
rv = tokenizer.fit(training)
rv2 = hashingTF.transform(rv)
In [92]: type(rv2)
Out[92]: pyspark.sql.dataframe.DataFrame
In [90]: rv2.printSchema()
root
 |-- id: long (nullable = true)
 |-- text: string (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- features: vector (nullable = true)

#+END_SRC
** metadata: 
http://stackoverflow.com/questions/32628845/is-there-a-way-to-add-extra-metadata-for-spark-dataframes
columns in dataframe can have metadata 
#+BEGIN_SRC python
In [18]: df = spark.createDataFrame(
    ...:     [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c
    ...: ")],
    ...:     ["id", "category"])
    ...: 

In [19]: indexer = StringIndexer(inputCol="category", outputCol="categ
    ...: oryIndex")

In [20]: model = indexer.fit(df)
    ...: indexed = model.transform(df)
    ...: 

In [21]: indexed.show()
    ...: 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+

In [23]: indexed.schema.fields
Out[23]: 
[StructField(id,LongType,true),
 StructField(category,StringType,true),
 StructField(categoryIndex,DoubleType,true)]

In [26]: indexed.printSchema()
root
 |-- id: long (nullable = true)
 |-- category: string (nullable = true)
 |-- categoryIndex: double (nullable = true)


In [24]: [t.metadata for t in indexed.schema.fields]
Out[24]: 
[{},
 {},
 {u'ml_attr': {u'name': u'categoryIndex',
   u'type': u'nominal',
   u'vals': [u'a', u'c', u'b']}}]


#+END_SRC
** Chain operation 
Note the use of the df.select(..).withColumn(..) and col(""). About the use of col("exp") function: not needed if exp is just a column name, but needed if more complicated expression (Cf. https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/sql/Column.html) 
#+BEGIN_SRC python 
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
sentenceDataFrame = spark.createDataFrame([
    (0, "Hi I heard about Spark"),
    (1, "I wish Java could use case classes"),
    (2, "Logistic,regression,models,are,neat")
], ["id", "sentence"])
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
regexTokenizer = RegexTokenizer(inputCol="sentence", outputCol="words", pattern="\\W")
# alternatively, pattern="\\w+", gaps(False)
countTokens = udf(lambda words: len(words), IntegerType())
tokenized = tokenizer.transform(sentenceDataFrame)
tokenized.select("sentence", "words")\
    .withColumn("tokens", countTokens(col("words"))).show(truncate=False)
regexTokenized = regexTokenizer.transform(sentenceDataFrame)
regexTokenized.select("sentence", "words") \
    .withColumn("tokens", countTokens(col("words"))).show(truncate=False)

#+END_SRC

** common functions: 
#+BEGIN_SRC python 
# number of rows, 
df.count() #return number of row regardless NA or not(vs. Pandas) 

# number of columns
len(df.columns)

# show the first row  
df.first()

#only take the first n row; 
#df.limit() return a new dataframe while head()/take() return a list of Row
df.limit(1).collect() [Row(age=2, name=u'Alice')]

# show unique values of a column 
df.select('col_name').distinct().show()

#split a dataset 
df_train, df_test = df.randomSplit([0.8,0.2])

#Stratified sampling: 
>>> from pyspark.sql.functions import col 
>>> dataset = sqlContext.range(0, 100).select((col("id") % 3).alias("key")) 
>>> sampled = dataset.sampleBy("key", fractions={0: 0.1, 1: 0.2}, seed=0) 
>>> sampled.groupBy("key").count().orderBy("key").show() 

#convert  row of vectors to np.array, /d/ below is a Row
w2v_plot = np.array(w2v.rdd.map(lambda d: d.vector.toArray()).take(100))

#+END_SRC
* Spark MLlib
** Pipeline
*** Summary
+ inspired by /sklearn/, and relied on Dataframe.  
+ Transformers, Estimators, Evaluator
Transformer.transform()s and Estimator.fit()s are both stateless. /Model/ is a transformer.  
+ Pipeline can be nonlinear (DAG) 
+ Same instance cannot be inserted into the pipeline twice(but can be two instance of the same type)
+ PipelineModel consists of multiple transformers and estimators(Cf. Pipeline.scala) 
#+BEGIN_SRC python
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

# Prepare training documents from a list of (id, text, label) tuples.
training = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0)
], ["id", "text", "label"])

# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# Fit the pipeline to training documents.
model = pipeline.fit(training)

# Prepare test documents, which are unlabeled (id, text) tuples.
test = spark.createDataFrame([
    (4, "spark i j k"),
    (5, "l m n"),
    (6, "spark hadoop spark"),
    (7, "apache hadoop")
], ["id", "text"])

# Make predictions on test documents and print columns of interest.
prediction = model.transform(test)
selected = prediction.select("id", "text", "probability", "prediction")
for row in selected.collect():
    rid, text, prob, prediction = row
    print("(%d, %s) --> prob=%s, prediction=%f" % (rid, text, str(prob), prediction))
#+END_SRC
*** vs. Pipeline in sklearn 
sklearn Pipeline consists of both transformers and estimators. Transformers in sklearn implement both /fit/ and /transfrom/ method while estimators in sklearn implements /fit/ and /predict/ methods. Also, transformers/estimators in sklearn might has states while /transformers/ and /estimators/ in Spark are *stateless*. /estimators/ in sklearn rely on its internal state for prediction while /estimators/ in spark returns a /model/(which is a transformer)
Cf. [[https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/pipeline.py#L86][Pipeline:_validate_steps]]
Cf. [[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html][FunctionTransformer]]
Cf. [[http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator][rolling your own estimator]]

*** Param and ParamMap
+ MLlib /Estimators/ and /Transformer/s use a uniform API for specifying parameters.
+ Parameters belong to specific instances of Estimators and Transformers. For example, if we have two LogisticRegression instances lr1 and lr2, then we can build a ParamMap with both maxIter parameters specified: ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20).
#+BEGIN_SRC 
# to see the parameters for an estimator 
In [33]: lr = LogisticRegression(maxIter=10, regParam=0.01)
In [34]: print("LogisticRegression parameters:\n" + lr.explainParams()
    ...:  + "\n")
In [39]: lr.maxIter
Out[39]: Param(parent=u'LogisticRegression_4f8190d0025ad21226c0', name='maxIter', doc='max number of iterations (>= 0).')

n [41]: paramMap = {lr.maxIter: 20}
In [42]: paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) 
In [43]: paramMap2 = {lr.probabilityCol: "myProbability"}
In [44]: paramMapCombined = paramMap.copy()
In [45]: paramMapCombined.update(paramMap2)
In [46]: model2 = lr.fit(training, paramMapCombined)

# extractParamMap() is not working in PySpark https://issues.apache.org/jira/browse/SPARK-13368
print(model2.extractParamMap()) 
#+END_SRC

*** Cross Validation 
After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset.
Q: how to extract the best model?
A: /cvModel.bestModel/; if /cvModel/ is a Pipeline model, each stage can be retrieved by /cvModel.bestMode.stages[i]/ 
Q: how to extract the best parameters from cvModel
A: look at: /cvModel.getEstimatorParamMaps()/ and /cvModel.avgMetrics/ 
#+BEGIN_SRC python 
#print the metrics for each combination of parameters in cv search 
paramNames_pp=[";".join(["%s:%g" % (k.name,v) for k,v in p.items()]) for p in cvModel.getEstimatorParamMaps()]
print(zip(paramNames_pp, cvModel.avgMetrics))
#+END_SRC

** Vector and Array 
#+BEGIN_SRC python 
In [94]: v1
Out[94]: DenseVector([0.0, 1.1, 0.1])
In [95]: v2 = 
KeyboardInterrupt
In [95]: v2 = Vectors.dense([0.0, 1.1, -.1])
In [96]: rv = sc.parallelize([v1,v2])
In [97]: rv
Out[97]: ParallelCollectionRDD[149] at parallelize at PythonRDD.scala:475
#+END_SRC
*** Sparse Vector and Sparse matrix 
#+BEGIN_SRC python 
from pyspark.mllib.linalg import Vectors
import scipy.sparse as sps 
sv1 = Vectors.sparse(3,[0,2],[1.0,3.0]) # the first integer is the size
sv2 = sps.csc_matrix(..)
#+END_SRC
SVD 
#+BEGIN_SRC scala
//From the book of Learning Spark 
val svd = mat.computeSVD(3,computeU=true) //the type of svd is SingularValueDecomposition[RowMatrix,Matrix] 
val s: Vector = svd.s
val V: Matrix = svd.V #V is a local matrix.for the first k svd components, V is of size n by k 
#+END_SRC

*** Matrix multiplcation 
Rf. /Learning Spark/
+ Local vector/matrix depends on the API. for Python, we can usually directly pass numpy array or use the /pyspark.mllib.linalg.Vectors/ which is a wrapper round numpy. In particular, we can get the numpy array from pyspark array using the /toArray/ method ; for Scala/Java, it's /org.apache.spark.mllib.linalg.Vectors/ which are wrappers around /Breeze/ array. spark mllib around are primarily meant for data representation and do not provide arithmetic operations such as addition and subtraction. 
+ Distributed matrices are wrappers around RDD[]. 
+ Note that the indices of /IndexedRowMatrix/ might not be consecutive(http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix). When a /IndexedRowMatrix/ multiplies a local matrix, it converts to /RowMatrix/ and then zip back the indices. 
#+BEGIN_SRC scala
scala> val idx = "^v([0-9]+)$".r
idx: scala.util.matching.Regex = ^v([0-9]+)$
scala> val rdd = sc.parallelize(List(
      (("v1","v1"),2),(("v1","v2"),4),  
      (("v1","v3"),1),(("v2","v2"),5),  
      (("v2","v3"),1),(("v3","v3"),2)   
      ))                                
scala> val mat = new CoordinateMatrix(rdd.map {case ((idx(i),idx(j)),v) =>
      MatrixEntry(i.toLong-1,j.toLong-1,v.toDouble) 
      }).toIndexedRowMatrix                         
                                                  

//we can multiply a distributed matrix by a local matrix(only in Scala API, not Python)
val vector = Matrices.dense(3,1,Array(4.0,1.0,5.0))
mat.multiply(vector).rows.map(println).collect()

//or we can do it in map-reduce way 
val vec = sc.parallelize(List(("v1",4),("v2",1),("v3",5)))
val t = (rdd.map {case ((i,j),v) => (j,(i,v))} join vec).values map {case ((i,v1),v2) => (i,v1*v2)} reduceByKey(_+_)

//If we want to multiply two distributed matrices, both of them needs to be block matrix. 


#+END_SRC
** Feature Engineering 
*** Tokenizer 
*** TF: 
+ Hashing TF:
Why number of buckets is advisable to be 2^k?
To read: https://en.wikipedia.org/wiki/Feature_hashing
+ ConutVectorizer:
/CountVectorizer/ is an /estimator/. During the fitting process, CountVectorizer will select the top /vocabSize/ words ordered by term frequency across the corpus. CountVectorizer.fit() generates a /CountVectorizerModel/. /CountVectorizerModel::transform(df_test)/ counts the term frequency in /df_test/ according to the /vocabulary/; the words not in the /vocabulary/ are ignored.  
#+BEGIN_SRC python
In [2]: df = spark.createDataFrame([
   ...:     (0, "a b c".split(" ")),
   ...:     (1, "a b b c a".split(" "))
   ...: ], ["id", "words"])

In [3]: cv = CountVectorizer(inputCol="words", outputCol="features", vocabSize=3, minDF=2.0)

In [4]: model = cv.fit(df)

In [5]: model.vocabulary
Out[5]: [u'b', u'a', u'c']

In [9]: df_test = spark.createDataFrame([(0, "c c c d a".split(" "))],["id","words"])

In [10]: df_test.show()
+---+---------------+
| id|          words|
+---+---------------+
|  0|[c, c, c, d, a]|
+---+---------------+


In [11]: model.transform(df)
Out[11]: DataFrame[id: bigint, words: array<string>, features: vector]

In [12]: model.transform(df).show(truncate=True)
+---+---------------+--------------------+
| id|          words|            features|
+---+---------------+--------------------+
|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|
|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|
+---+---------------+--------------------+


In [13]: model.transform(df).show(truncate=False)
+---+---------------+-------------------------+
|id |words          |features                 |
+---+---------------+-------------------------+
|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|
|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|
+---+---------------+-------------------------+


In [14]: model.transform(df_test).show(truncate=False)
+---+---------------+-------------------+
|id |words          |features           |
+---+---------------+-------------------+
|0  |[c, c, c, d, a]|(3,[1,2],[1.0,3.0])|
+---+---------------+-------------------+

#+END_SRC
*** IDF: 
Note that IDF doesn't normal the vector(vs. TfidfVectorizer in sklearn)
*** Word2Vec
The tutorial is misleading regarding to this example. sample_lda_data.txt is the training data for this example, instead of text8 data(even though this is the standard training set for word2vec). http://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/feature.html#Word2Vec
#+BEGIN_SRC python
from pyspark.mllib.feature import Word2Vec

#sgu: 
inp = sc.textFile("data/mllib/sample_lda_data.txt").map(lambda row: row.split(" "))

word2vec = Word2Vec()
model = word2vec.fit(inp)

synonyms = model.findSynonyms('1', 5)

for word, cosine_distance in synonyms:
    print("{}: {}".format(word, cosine_distance))
#+END_SRC
*** StringIndexer, Index2String  
+ StringIndexer can be used to turn string into numeric index. As Spark 2.1.0, classifiers cannot handle string label so StringIndexer is needed  
#+BEGIN_SRC python 
In [13]: df = spark.createDataFrame(
    ...:     [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c
    ...: ")],
    ...:     ["id", "category"])
    ...: 

In [14]: indexer = StringIndexer(inputCol="category", outputCol="categ
    ...: oryIndex")

In [15]: indexed = indexer.fit(df).transform(df)

In [16]: indexed.show()
    ...: 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+


In [17]: indexed.printSchema()
root
 |-- id: long (nullable = true)
 |-- category: string (nullable = true)
 |-- categoryIndex: double (nullable = true)

#+END_SRC

+ Q: how to map the index back to string? 
We can use the meta data in the column created by StringIndexer to convert numeric column back to string. 
#+BEGIN_SRC python 
    #Map back the predict index to label name if needed 
    df, df_pred = documents, df_train 
    name_mapping = dict([(col,t.metadata) for col, t in zip(df.columns,df.schema.fields)])['label_n']['ml_attr']['vals']
    df_pred = IndexToString(inputCol='label_pred_n',outputCol='label_pred',labels = name_mapping).transform(df_pred)
#+END_SRC

** text segmentation
Stanford NLP Group, scalanlp/chalk 
*** Word2Vec
Estimator also. 
http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec
** Classification
*** What's the rawPrediction as in LogisticRegression ctor?
For LogisticRegression, it's the logit (the linear function value, equiv. to  /prediction(glm_fit,new_data type="link") in R/ 
http://stackoverflow.com/questions/37903288/what-do-colum-rawprediction-and-probability-of-dataframe-mean-in-spark-mllib
*** What's aggregationDepth param in LogisticRegression ctor? 
This is related to an improvement of MLlib regarding to MapReduce Paradigm. Multi-level aggregation tree is used to combined partially on a small set of executors before they sent to the drivers which is dramatically reduces the load of the driver. https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html
Note this WIP: https://issues.apache.org/jira/browse/SPARK-17090

* AWS-EC2
** Boto Script 
#+BEGIN_SRC python 
import boto.ec2
conn = boto.ec2.connect_to_region("us-east-1")
#conn.get_all_security_groups()[1].id #u'sg-7ea8bf1a'
reservation = conn.run_instances("ami-d05e75b8",key_name = "sgu-ec2-all", instance_type = "t2.micro", security_groups=["launch-wizard-1"]) #aws_access_key is in ~/.boto or set in env or pass in directly
reservations = conn.get_all_reservations()
inst = reservations[0].instances[0] #get the instance, inst? show all attribute, include the ip_address, type, ect. 
statuses = conn.get_all_instance_status()
#make sure instance is available
conn.terminate_instances(instance_ids = [inst.id])
#+END_SRC 
** system_status check vs. instance_status check 
system_status is for the underlying EC2 system that are used by each individual  instance. System status problem requires AWS involvement to repair. Instance status is for individual instances. The customer can fix instance status problems.  
** Q: EBS device mapping? instance_profile? initiliazation script(user-data)
** Q: how to use VPC
After 2013/12, each account comes with a default VPC which has a default subnet in each availability zone. If we don't specify a subnet when launching an intance, the instance is launch into the default VPC and default subnet.  We can then create a *security group* to manage the network traffic. 
http://docs.aws.amazon.com/cli/latest/reference/ec2/authorize-security-group-ingress.html
http://stackoverflow.com/questions/24232849/source-security-group-isnt-working-as-expected-in-aws
https://aws.amazon.com/vpc/

** Q: How to run a script after the instance starts?
Use the user-data kwarg in /image.run(..)/
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html
https://imiloainf.wordpress.com/2012/01/04/use-boto-run-user-data-in-aws-instance/
#+BEGIN_SRC python
import boto
from boto.ec2.connection import EC2Connection
import time
 
ami_id = '----'                                                  # ami-ubuntu-64 (settedup) use t1.micro as instance type
key_pair_name = '----'
AWS_ACCESS_KEY_ID = '----'
AWS_SECRET_ACCESS_KEY = '----'
 
ec2conn = EC2Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
 
test_script_00 = """#!/bin/bash
sudo apt-get update
sudo apt-get install imagemagick
"""
 
test_script_01 = """#!/usr/bin/env python
 import boto
 from boto.s3.key import Key
 
AWS_ACCESS_KEY_ID = '-------'
AWS_SECRET_ACCESS_KEY = '----'
 
bucket_name = 'demo-test'
conn_s3 = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)
bucket = conn_s3.get_bucket(bucket_name)
 
k=Key(bucket)
k.key='images/rose111105.jpg'
k.copy('demo-test', 'images/rose111105copy6.jpg')
"""
 
test_script ="""#!/usr/bin/env python
import smtplib
import boto
from boto.s3.key import Key
import numpy as np
import sys
 
AWS_ACCESS_KEY_ID = '-----' # Your AWS_KEY
AWS_SECRET_ACCESS_KEY = '---------' # Your Secret KEY
 
def send_notice(msg='testing'):
    fromaddr = '******@gmail.com'
    toaddrs = '******@gmail.com'
    username = '******'
    password = '******'
     
    server = smtplib.SMTP('smtp.gmail.com:587')
    server.starttls()
    server.login(username,password)
    server.sendmail(fromaddr, toaddrs, msg)
    server.quit()
 
send_notice("start processing")
#DO YOUR PROCESS
send_notice("Finishing...")
"""
 
my_reservation = ec2conn.run_instances(ami_id,
                                       instance_type=instance_type,
                                       key_name=key_pair_name,
                                       user_data=test_script)
 
instance = my_reservation.instances[0]
while not instance.update() == 'running':
    time.sleep(5)
instance.stop()
#+END_SRC
* Setting up Spark-EC2

** dissect spark_ec2.py
./spark-ec2 -k sgu-ec2-all -i $HOME/.ssh/sgu-ec2-all.pem -s 2 launch spark_test  
-s: the number of slave 
'launch': the action 
'spark_test': cluster_name 
/cluster_name/ is used to identify the security group.   
*** launch_cluster
1. get or create security group /cluster_name/-master , /cluster_name/-slaves
2. authorize the security group (handle vpc)  so that master can access slaves
3. check whether the cluster has already exist /get_exsiting_cluster/
4. get with AMI with Spark 
   which looks up git-repos based on region and instance type: eg. 'https://raw.github.com/amplab/spark-ec2/branch-2.0/ami-list/us-east-1/pvm'
5. create EBS device mapping so that we can add EBS volumne if asked to (vol.size, vol.type, attached_name(/dev/sds,/dev/sdt,ect.), delete_on_termination).
6. launch slave instance or request spot instances (the instances is collected in /slave_nodes/
7. launch master instance(only one, not a spot instance)
8. tag the master and slaves 
9. return /master_nodes/ and /slave_nodes/
*** get_existing_cluster
return /master_instance/, /slave_instances/
find /cluster_name/+'-master' and /cluster_name/+'-slaves'
*** wait_for_cluster_state
make sure all master_nodes, and slave_instances are ssh-ready. 

*** setup_cluster
Q: What is swap space (specified by input argument =-swap=)?
Q: what is tachyon?   
1. on master: create a ssh key and copy to the slaves. 
2. clone spark-ec2 git-repos 
3. prepare a local file with all necessary parameters filled  and then copy the file (rsync) to master. 
4.  deploy other user files (parameter /deploy_root_dir/) to master
5. run /setup_spark_cluster/, which run spark-ec2/setup.sh which fills in the package config files under /spark-ec2/templates/ and write the conf to each individual package. Note eventually after a package is setup, part of the package is from other site(eg. S3) and part of it is from the /templates/ directory.  
 


** A simple Version
#+BEGIN_SRC python 
import boto
from boto.ec2.blockdevicemapping import BlockDeviceMapping, BlockDeviceType, EBSBlockDeviceType
from boto import ec2

conn = ec2.connect_to_region("us-east-1")

#for region: us-east-1, instance type: pvm;
# given in 'https://raw.github.com/amplab/spark-ec2/branch-2.0/ami-list/us-east-1/pvm'
# Amazon Linux, username: root. 
ami_name = 'ami-5bb18832'
image = conn.get_image(ami_name)

#launch only one instance 
#there are more kwargs to launch multiple intance, attached EBS, run a configure script, ect. 
#need to specify aws_access_key_id and aws_secret_access_key in ~/.boto or in evironment
# the private pem key associated with the key_name is needed to ssh into the instance later.
# return a reservation
res = image.run(key_name = "sgu-ec2-all",
                instance_type = "m1.large",
                security_groups=["launch-wizard-1"])

time.sleep(20)   #wait for 20 secs 
res = [s for s in conn.get_all_reservations() if s.id == res.id][0]
print "ssh -i path-to-pem root@%s" % (res.instances[0].ip_address,)

#+END_SRC
* HDFS 
** UI: 
http://{host}:50070
 
** configuration
*** Q: What is the default config dir? 
A: If the env /HADOOP_CONF_DIR/ is not defined, then /HADOOP_CONF_DIR/ is set to ${HADOOP_PREFIX}/conf or ${HADOOP_PREFIX}/etc/hadoop (see the logic in libexec/hadoop-config.sh
*** Default values 
+ the default value of /dfs.data.dir/, /dfs.name.dir/ are based on /hadoop.tmp.dir/(in etc/hadoop/core-site.xml). Cf. https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
** /hdfs getconfig/
#+BEGIN_SRC shell
hdfs getconf -confKey dfs.blocksize
#+END_SRC
** Debug and Monitor 
*** /jps/
the jdk /jps/ command list all the daemon. 
#+BEGIN_SRC shell
[hadoop-user@master]$ jps
26893 Jps
26832 TaskTracker
26620 SecondaryNameNode
26333 NameNode
26484 DataNode
26703 JobTracker

#+END_SRC

*** /netstat/
#+BEGIN_SRC shell
#-p: PID; l: listening u: TCP; e: extended info; n: numeric port 
hduser@ubuntu:~$ sudo netstat -plten | grep java
tcp   0  0 0.0.0.0:50070   0.0.0.0:*  LISTEN  1001  9236  2471/java
tcp   0  0 0.0.0.0:50010   0.0.0.0:*  LISTEN  1001  9998  2628/java
tcp   0  0 0.0.0.0:48159   0.0.0.0:*  LISTEN  1001  8496  2628/java
tcp   0  0 0.0.0.0:53121   0.0.0.0:*  LISTEN  1001  9228  2857/java
tcp   0  0 127.0.0.1:54310 0.0.0.0:*  LISTEN  1001  8143  2471/java
tcp   0  0 127.0.0.1:54311 0.0.0.0:*  LISTEN  1001  9230  2857/java
tcp   0  0 0.0.0.0:59305   0.0.0.0:*  LISTEN  1001  8141  2471/java
tcp   0  0 0.0.0.0:50060   0.0.0.0:*  LISTEN  1001  9857  3005/java
tcp   0  0 0.0.0.0:49900   0.0.0.0:*  LISTEN  1001  9037  2785/java
tcp   0  0 0.0.0.0:50030   0.0.0.0:*  LISTEN  1001  9773  2857/java

#+END_SRC
** /fsck/ 
fs checking utility 
#+BEGIN_SRC 
# Show the block size of a file
# We can see more details of a block in the web UI Utilities tab 
hdfs /test/text8_2 -files -blocks 
#+END_SRC
** /put/ with blocksize
#+BEGIN_SRC 
#specify block size 
hadoop fs -Ddfs.blockzise -put /path/file1 /path/file2  
#+END_SRC
