#+BLOG: wordpress
#+POSTID: 123
-----------------------
#+TITLE: Study Note: Tensorflow
#+DATE:  <2017-01-07 Sun>
#+CATEGORY: Machine Learning
-----------------------
* Tensor vs. Variable, Constant, Node
+ Base on [[http://stackoverflow.com/questions/37849322/how-to-understand-the-term-tensor-in-tensorflow]], /There is no notion of Tensor in the underlying graph that's executed by the runtime/. Instead the Graph consists of op nodes connected to eacth other, representing operations. An operation allocates memory for its outputs which are available on endpoints :0, :1, ect, and you an think of each of these endpoints as a Tensor. If we want to extend tensorflow, we need to define new operations during which we deal with tensors: https://www.tensorflow.org/how_tos/adding_an_op/. Furthermore, a tensor can be referred to through op:port, 
#+BEGIN_SRC python 
    #code excerpt from https://github.com/tensorflow/tensorflow/issues/6322
    # Hidden 1
    with tf.name_scope('hidden1'):
      weights = tf.Variable(
          tf.truncated_normal([IMAGE_PIXELS, hidden1_units],
                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),
          name='weights')
       biases = tf.Variable(tf.zeros([hidden1_units]),name='biases')
    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)
    //....
    hidden1_outputs = tf.get_default_graph().get_tensor_by_name('hidden1/add:0')    
#+END_SRC

+ Variables is an operation. We can think of a variable as representing trainable parameters. http://stackoverflow.com/questions/38556078/in-tensorflow-what-is-the-difference-between-a-variable-and-a-tensor. However, we can also define non-trainable Variable like in the following use case
#+BEGIN_SRC python 
global_step = tf.Variable(0, name='global_step', trainable=False)
train_op = optimizer.minimize(loss, global_step=global_step)
#+END_SRC
However, the "Variables" referred in the tensorflow doc are not the same as memory holding values (as in programming language). In a tf NN, many other tensors are not "Variable". For example, =hidden1_outputs= in the snippet above is not a variable. To get all variables in the graph, 
#+BEGIN_SRC python 
#for the MNIST network in tutorial: 
In [2]: lv = tf.get_collection(tf.GraphKeys.VARIABLES)
In [4]: [v.name for v in lv]
Out[4]: 
[u'hidden1/weights:0',
 u'hidden1/biases:0',
 u'hidden2/weights:0',
 u'hidden2/biases:0',
 u'softmax_linear/weights:0',
 u'softmax_linear/biases:0',
 u'global_step:0']

In [13]: lvt = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
In [14]: [v.name for v in lvt]
Out[14]: 
[u'hidden1/weights:0',
 u'hidden1/biases:0',
 u'hidden2/weights:0',
 u'hidden2/biases:0',
 u'softmax_linear/weights:0',
 u'softmax_linear/biases:0']
#+END_SRC

+ The term Tensor and Variable are used differently in Python and C++ API. 
https://stackoverflow.com/questions/40866675/implementation-difference-between-tensorflow-variable-and-tensorflow-tensor

+ Why use tf.constant()? 
For efficiency, readability(of the graph). 
http://stackoverflow.com/questions/39512276/tensorflow-simple-operations-tensors-vs-python-variables

* Evaluating graph element 
** Evaluating an operation vs. evaluating a variable 
When we run an operation  =sess.run(op)=, every other operation which /op/ depends on is also  re-evaluated. Recall there is no notion of tensor -- Tensor is just an output of an operation hence evaluating a tensor is the same as  evaluating an operation. In contrast, evaluating a var is simply fetching its value. If we want to change the variable, we need to use tf.assign which is an operation. The following code, demonstrate the difference of evaluating an op vs. evaluating a variable.   
#+BEGIN_SRC python 
import tensorflow as tf
flags = tf.flags
logging = tf.logging
flags.DEFINE_string("save_path", None,
                    "Model output directory.")
FLAGS = flags.FLAGS

def main(_):
    with tf.Graph().as_default():
        epoch_size = tf.constant(10)
        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue(name="i") #i changes each time it is evaluated, 
        #v = i + 1
        v = tf.Variable(0,trainable = False)
        op = tf.assign(v,i)
        v3 = v + 1
        sv = tf.train.Supervisor(logdir=FLAGS.save_path)
        #sgu: sv.managed_seesion() instead of tf.Session() is needed for tf.train.range_input_producer to work; tf.Session() hangs. 
        with sv.managed_session() as session:
            t = tf.get_default_graph().get_tensor_by_name("i:0")
            session.run(op) #op depends on i, i is re-evaluated when op is run and hence v is changed every time op run. v=0 after this run 
            session.run(op) #v=1 after this run 
            for _ in range(10):
                #v is a variable, it doesn't change unless the assign op runs.  
                v_val = session.run(v) 
                print "v_val=", v_val
                #v3 depends only on v, v3 doesn't change since v doesn't change.
                v3_val = session.run(v3) 
                print "v3_val=",v3_val
                            
if __name__ == "__main__":
    tf.app.run()
#+END_SRC
** evaluating sequentially vs. evaluating simultaneously
session.run([a,b]) accepts a list of graph elements and will evaluate the a,b in one run. This is different than session.run(a) followed by a session.run(b) 
#+BEGIN_SRC python 
import tensorflow as tf
flags = tf.flags
logging = tf.logging
flags.DEFINE_string("save_path", None,
                    "Model output directory.")
FLAGS = flags.FLAGS
def main(_):
    with tf.Graph().as_default():
        #i changes each time it is evaluated, 
        i = tf.train.range_input_producer(10, shuffle=False).dequeue(name="i") 
        v = i + 1
        v3 = i + 1
        sv = tf.train.Supervisor(logdir=FLAGS.save_path)
        #sgu: sv.managed_seesion() instead of tf.Session() is needed for tf.train.range_input_producer to work; tf.Session() hangs. 
        with sv.managed_session() as session:
            for _ in range(10):             
                v_val, v3_val = session.run([v,v3]) #line A
                print "v=%d,v3=%d" % (v_val,v3_val) 
                                           
if __name__ == "__main__":
    tf.app.run()

Running the code above gives: 
v=1,v3=1
v=2,v3=2
v=3,v3=3
v=4,v3=4
v=5,v3=5
v=6,v3=6
v=7,v3=7
v=8,v3=8
v=9,v3=9
v=10,v3=10
#+END_SRC
In contrast, if we evaluate /v/,/v3/ sequentially(i.e.,  replace line A), we have different answer. 
#+BEGIN_SRC python
import tensorflow as tf
flags = tf.flags
logging = tf.logging
flags.DEFINE_string("save_path", None,
                    "Model output directory.")
FLAGS = flags.FLAGS

def main(_):
    with tf.Graph().as_default():
        epoch_size = tf.constant(10)
        i = tf.train.range_input_producer(10, shuffle=False).dequeue(name="i") #i changes each time it is evaluated, 
        v = i + 1
        v3 = i + 1
        sv = tf.train.Supervisor(logdir=FLAGS.save_path)
        #sgu: sv.managed_seesion() instead of tf.Session() is needed for tf.train.range_input_producer to work; tf.Session() hangs. 
        with sv.managed_session() as session:
            for _ in range(10):
                v_val = session.run(v) 
                v3_val = session.run(v3) 
                print "v=%d,v3=%d" % (v_val,v3_val) 

if __name__ == "__main__":
    tf.app.run()

run the code above gives: 
v=1,v3=2
v=3,v3=4
v=5,v3=6
v=7,v3=8
v=9,v3=10
v=1,v3=2
v=3,v3=4
v=5,v3=6
v=7,v3=8
v=9,v3=10

#+END_SRC

* The shape of tensors 
 The first dimension of the input tensor and output tensor is the batch size. 
Cf.  https://stackoverflow.com/questions/39090222/tensorflow-single-value-vs-batch-tensors
In the MNIST example, the input tensor is of shape (100, 784) where 100 is the batch size. The output tensor is of shape (100,) since the target ($y$) is just one number indicating the class (0-9). Since input data is fed by a batch in each iteration, it's typical to apply reduction when computing loss ( =tf.reduce_mean=, as shown below). The first dimension of the tensors of the hidden layer also matches the batch size(see the example below)  
#+BEGIN_SRC python 
#examples/tutorials/mnist/fully_connected_feed.py
def placeholder_inputs(batch_size):
  """Generate placeholder variables to represent the input tensors.

  These placeholders are used as inputs by the rest of the model building
  code and will be fed from the downloaded data in the .run() loop, below.

  Args:
    batch_size: The batch size will be baked into both placeholders.

  Returns:
    images_placeholder: Images placeholder.
    labels_placeholder: Labels placeholder.
  """
  # Note that the shapes of the placeholders match the shapes of the full
  # image and label tensors, except the first dimension is now batch_size
  # rather than the full size of the train or test data sets.
  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,
                                                         mnist.IMAGE_PIXELS))
  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))
  return images_placeholder, labels_placeholder

feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)

In [1]: feed_dict
Out[1]: 
{<tf.Tensor 'Placeholder:0' shape=(100, 784) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        ..., 
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),
 <tf.Tensor 'Placeholder_1:0' shape=(100,) dtype=int32>: array([5, 6, 5, 1, 3, 1, 2, 0, 9, 3, 5, 1, 9, 2, 2, 3, 6, 5, 4, 1, 6, 4, 9,
        9, 0, 0, 2, 8, 9, 2, 9, 9, 5, 9, 9, 4, 3, 7, 8, 5, 5, 1, 8, 5, 0, 3,
        8, 8, 1, 9, 3, 5, 0, 3, 2, 5, 6, 3, 6, 5, 7, 8, 7, 0, 8, 1, 6, 3, 3,
        4, 0, 8, 7, 7, 7, 5, 7, 6, 0, 5, 7, 5, 1, 3, 6, 0, 1, 1, 7, 7, 5, 5,
        1, 0, 3, 0, 9, 5, 0, 4], dtype=uint8)}

def loss(logits, labels):
  """Calculates the loss from the logits and the labels.

  Args:
    logits: Logits tensor, float - [batch_size, NUM_CLASSES].
    labels: Labels tensor, int32 - [batch_size].

  Returns:
    loss: Loss tensor of type float.
  """
  labels = tf.to_int64(labels)
  # sgu: cross_entropy.get_shape(): TensorShape([Dimension(100)]) where 100 is the batch_size 
  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
      labels=labels, logits=logits, name='xentropy') 
  return tf.reduce_mean(cross_entropy, name='xentropy_mean')

#sgu: due to the tf.reduce_mean, loss now is just a number(i.e., 0-D tensor)
loss = mnist.loss(logits, labels_placeholder) #loss.get_shape(): TensorShape([])

# Get the outputs before the ReLU.
# hidden1_outputs.get_shape(): TensorShape([Dimension(100), Dimension(128)])
# where 100 is the batch size and 128 is the dimension in the hidden1 layer,
# i.e., 128 features are extracted in hidden1
hidden1_outputs = tf.get_default_graph().get_tensor_by_name('hidden1/add:0') 
#+END_SRC

In some case, the input tensor might be of rank 1 (=train_inputs=) and the output tensor (=train_labels=) would be rank 2.  This is not a problem as long as the operations can consume them. In the sample code below, =tf.nn.embedding_lookup= and  =tf.nn.nce_loss= require the arguments of those shapes.  
#+BEGIN_SRC python 
#example/tutorials/word2vec/word2vec_basic.py. 
    #sgu train_inputs.get_shape(): TensorShape([Dimension(32)]) where 32 is the batch_size 
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) #nce_loss requires train_lables to be rank 2 
    embeddings = tf.Variable(
               tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

    #sgu embed.get_shape(): TensorShape([Dimension(50000), Dimension(128)]) where 50000 is the vocab size and 128 is the embedding_size. 

    #sgu:embed.get_shape(): TensorShape([Dimension(32), Dimension(128)]) 
    #sgu: where 32 is the batch_size and 128 is the dimension size
    embed = tf.nn.embedding_lookup(embeddings, train_inputs) 

    # Construct the variables for the NCE loss
    nce_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

  #sgu: loss is a scalar, ie, a tensor of rank 0
  #loss.get_shape: TensorShape([])
  loss = tf.reduce_mean(
      tf.nn.nce_loss(weights=nce_weights,
                     biases=nce_biases,
                     labels=train_labels,
                     inputs=embed,
                     num_sampled=num_sampled,
                     num_classes=vocabulary_size))

#+END_SRC
* Add operation node to the graph 
The way the Tensorflow API is designed, library routines that create new operation nodes always attach nodes to the default graph. In fact, =value+1= or =reduce(value2)= adds new nodes to the graph (as demonstrated below). Interactive debugging needs to be careful not to create unintended new nodes to the graph. 
#+BEGIN_SRC python
In [1]: import tensorflow as tf
In [2]: graph=tf.get_default_graph()
In [3]: graph.as_graph_def()
Out[3]: 
versions {
  producer: 17
}
In [4]: value = tf.constant(1)
In [5]: graph.as_graph_def()
Out[5]: 
node {
  name: "Const"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
versions {
  producer: 17
}
In [6]: value2=value+1
In [7]: graph.as_graph_def()
Out[7]: 
node {
  name: "Const"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
node {
  name: "add/y"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
node {
  name: "add"
  op: "Add"
  input: "Const"
  input: "add/y"
  attr {
    key: "T"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 17
}
In [8]: tf.reduce_mean(value)
Out[8]: <tf.Tensor 'Mean:0' shape=() dtype=int32>
In [9]: graph.as_graph_def()
Out[9]: 
node {
  name: "Const"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
node {
  name: "add/y"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
node {
  name: "add"
  op: "Add"
  input: "Const"
  input: "add/y"
  attr {
    key: "T"
    value {
      type: DT_INT32
    }
  }
}
node {
  name: "Const_1"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
          }
        }
      }
    }
  }
}
node {
  name: "Mean"
  op: "Mean"
  input: "Const"
  input: "Const_1"
  attr {
    key: "T"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "keep_dims"
    value {
      b: false
    }
  }
}
versions {
  producer: 17
}

#+END_SRC
* Inspect the Graph
#+BEGIN_SRC python
# Q: what happen when name_scope is used together with variable_scope?
with tf.name_scope("ns"):
    with tf.variable_scope("vs"):
        v1 = tf.get_variable("v1",[1.0]) 
        v2 = tf.Variable([2.],name="v2")
        v3 = v1+v2
v1.name  #vs/v1:0
v2.name  #ns/vs/v1:0
v3.name  #ns/vs/add:0 

#list all the node
l = [n for n in tf.get_default_graph().as_graph_def().node] 
[(ll.name,ll.op) for  ll in l]

In [12]: g  = tf.get_default_graph()
In [13]: op = g.get_operation_by_name("ns/vs/add")
In [14]: op.node_def
Out[14]: 
name: "ns/vs/add"
op: "Add"
input: "vs/v1/read"
input: "ns/vs/v2/read"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
#get the output tensor of an op 
t = g.get_tensor_by_name("ns/vs/add:0") 
assert t==v3

#+END_SRC
* Name Scope vs. Variable Scope:
#+BEGIN_SRC python 
with tf.name_scope("ns"):
    with tf.variable_scope("vs"):
        v1 = tf.get_variable("v1",[1.0]) 
        v2 = tf.Variable([2.],name="v2")
        v3 = v1+v2
v1.name  #vs/v1:0
v2.name  #ns/vs/v1:0
v3.name  #ns/vs/add:0 
#+END_SRC
* Interactive Session vs. Session 
http://stackoverflow.com/questions/40645952/tensorflow-is-it-always-more-convenient-to-use-interactivesession-compared-to?noredirect=1&lq=1
* Sample code to visualization embedding vector 
This is an sample code to visualize MNIST hidden vector in Tensorboard
https://github.com/tensorflow/tensorflow/issues/6322
This is an sample code to visualize word2vec in Tensorboard:
https://github.com/shiyuangu/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py






