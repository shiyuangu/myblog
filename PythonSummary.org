-----------------------
#+TITLE: Python Summary 
#+DATE: 
#+CATEGORY: Programming
-----------------------
* Name Convention: 
  The name convention recommended by Python Enhancement Proposals(PEP)(Cf. [[http://legacy.python.org/dev/peps/pep-0008/][PEP8: Style Guide to Python Code ]]) is quite different from the name convention adopted by C++ community. In particular, 
  ** mixCase is discouraged. 
  **  Any identifier in  a class of the form =__spam= (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with leading underscore(s) stripped(Cf. [[https://docs.python.org/2/tutorial/classes.html#private-variables-and-class-local-references][Pydoc: Private Variables and Class-local References ]]).
  ** Any identifier in a class of the form =__getitem__= is a special method(Cf. [[https://docs.python.org/2/reference/datamodel.html#special-method-names][Special Methods]])
  ** A method in a module whose name starts with an underscore is not exported by default(i.e., =__all__=  is not defined) when `from mymodule import *` is invoked. Even when =__all__= is set appropriately, it's still considered to be a good practice to prefix the internal interface (packages, modules, classes, functions, attributes or other names) with a single leading underscore (Cf. [[http://legacy.python.org/dev/peps/pep-0008/#public-and-internal-interfaces][PEP8: public and private interface]]). 

* Modules: 
** Related commands/variables
     + show all loaded modules: sys.modules
** Modules are first class objects. 
     + It's a common practice to import a module in another module initialization file =__init__.py=. For example, Pandas imports numpy and hence we can refer numpy as pandas.np. However, this is not visible to user.  So users could import numpy again. In this case, all imported numpy binds to the same object and hence have the same id(...).
** All statements in the source file are executed. 

** Use =__all__= = ['bar','foo'] to precisely control the set of names import by from /module/ import =*= 
  
** Use /from module import foo/
   This will load the specific definitions into a current namespace. However, the binding rules for variables.  The global namespace for a function  is always the module in which the function is defined.
  #+BEGIN_SRC python 
  from spam import foo
  a = 42
  foo() # Prints "I'm foo and a is 37"
  #+END_SRC  
  ** The use of =__name__=:
  Each module defines a variable =__name__= with the module's name. The programs can exam this variable to determine the module in which they are executing. The top level module of the interpreter is called =__main__=.Programs specified in the command line or entered interactively run inside =__main=. The following program prints "in mymodule" when being executed by 'import mymodule' and print 'in __main__ ' when executed by `python mymodule.py`
   #+BEGIN_SRC python 
     ## mymodule.py
     if __name__ == '__main__':
        print "in __main__"
	 else:
	    print "in %s" % (__name__,) 
   #+END_SRC
** Use imp
#+BEGIN_SRC python
import imp 
imp.loan_source("config","/path/to/file.conf")
conf = __import__('config').configs  #config is a variable defind on the top level of file.conf 
#+END_SRC
* OOP:
** Unlike C++/Java, Base class _init_() is not automatically invoked:
 #+BEGIN_SRC python
        class Base(object):
            def __init__(self):
                print "In Base::init"
                self.a=1
        class Derived(Base):
            def __init__(self):
                print "In Derive::init"
                self.b=1
            def myprint(self):
                print "a=%d" % self.a
        obj = Derived()
        obj.myprint()   #error AttributeError: 'Derived' object has no attribute 'a'

 #+END_SRC
** In Python, the instance's attribute can be deleted.
 #+BEGIN_SRC python 
   class MyObject:
       """
       Test some pythonic OO features. 
       """
       a = 10;
       def __init__(self):
           self.b = 10;
       def f(self):
           print "this is f"
       
   obj = MyObject()
   del obj.a #error: only instance attribute can be deleted. 
   del obj.f #same error as above
   del obj.b #correct. 
 #+END_SRC

** The use of "@property": Uniform Access Principle. 
* Type conversion
  Speicial Methods __int__(), __float__() ect. are called by explicit conversion such as int(x), float(x), but never called implicitly to perform type conversion during mix-type arithmetic. 

* MetaClass:
In the following code, the descriptor TypedProperty doesn't hold the value directly. Instead, it holds the name of itself in the class. How the TypedProperty knows its own name in the class it belongs to? The magic in the Meta class. The variable num is a member of class Foo. When Foo is created by the meta class TypedMeta, the metaclass __new__(..) method has a dictionary, which is a map betweem the member name and the memeber. TypedMeta.__new__(..) changes the num's field name to num's name suffixed by an underscore.  Now, basically, the descriptor simply becomes a *proxy* to the underlining field "_num" which is stored in Foo itself. TypedMeta also use the __slots__ to forbid adding any new field to the class. 
#+BEGIN_SRC python
class TypedProperty(object):
    def __init__(self, type, default=None):
        self.name = None
        self.type = type
        if default: 
            self.default = default
        else:
            self.default = type()
    def __get__(self, instance, cls):
        print "self.name",self.name
        return getattr(instance,self.name, self.default)
    def __set__(self, instance, value):
        if not isinstance(value, self.type):
            raise TypeError("Must be a %s" % self.type)
        setattr(instance, self.name, value)
    def __delete__(self, instance):
        raise AttributeError("cannot delete attribute")
class TypedMeta(type):
    def __new__(cls, name, bases, dict):
        slots=[]
        print "TypedMeta::__new__.cls=%s,name=%s" % (cls,name)
        for key, value in dict.items():
            if isinstance(value, TypedProperty):
                value.name = "_"+key  #bind the memeber variabls's name to the  the descriptor 
                slots.append(value.name)
        dict['__slots__']=slots
        return type.__new__(cls,name,bases,dict)
class Typed:
    __metaclass__ = TypedMeta

class Foo(Typed):
    name = TypedProperty(str)
    num =  TypedProperty(int,42)
f=Foo()
f.num #output self.name _num, 42
f._num # AttributeError: _num; _num doesn't exist, not yet. 
f.num=44 # the descriptor's __set__ method is called which creates a new field _num
f._num  #44, now _num exists after a call to f.num
#+END_SRC

* Scoping Rules and Closure
 sgu:  There are two types of variables in a function: formal parameter and the other parameters, for example , in f(x) = a*x, x is the formal parameter. If a is bounded in the enclosing scope (not global scope, usually inside another function)  when the function f(x) is defined, f will has a closure and the value is remembered.. Closure is a function object which remembers the values regardless whether the enclosing scope is still present in memory. If  a is not bound in its immediate enclosing scope, the enclosing scope one level up, two level up before reaching the global scope  (regardless whether there is an 'a' in global scope), the name a will become a *free parameter* (https://docs.python.org/2/reference/executionmodel.html) and search in gloabl namespace. Note that in this case, f has no closure. Note that if there is no such name 'a' in global space at the time of *evaluation*, an error will be raised. If 'a' first exists but later is deleted, an error will also be raised. . Whether the parameter is found or not is determined by investigating f.__closure__[0].cell_contents, f.__closure__[1].cell_contents, ect. 
#+BEGIN_SRC python 
#this form a closure: 
def fp(): 
    a = 10 
    return lambda x: a*x 
g = fp() #g has a closure 
g.__closure__[0] 
 <cell at 0x7f84d0708868: int object at 0x1fa0080>

#nesting is ok for closure too: 
In [16]: def fp(): 
    ...:     a = 10 
    ...:     def fp2():
    ...:         f = lambda x: a*x 
    ...:         return f 
    ...:     return fp2()
    ...:     
    ...: 

In [17]: g = fp()  
In [18]: g.__closure__ #g has a closure 
Out[18]: (<cell at 0x7f84d0708280: int object at 0x1fa0080>,)

#however, class scope is not considered when forming a closure 
#this seems to be related to how local() work: free variable is returned when called in a function block but not in a class block: http://mathamy.com/python-closures-and-free-variables.html
In [37]: class Base: 
    ...:     b = 10 
    ...:     def __init__(self): 
    ...:         pass
    ...:     def g(self): 
    ...:         return lambda x: b*x

g = Base().g() 
g(2) #error, global name 'b' is not defined. 



#+END_SRC
#+BEGIN_SRC python 
b = 2
c = 1
class A:
    a = 1
    def myprint(self):
        print 'a=', self.a
        print 'b=', b
def myf(x):
    b = 2
    def myg(x):
        print 'b*x+c=', b*x+c  # only b is bounded, c is free 
    return myg

>> g = myf(10)
>> b = 3 #this doesn't change the binding in g
>> g(2) = 5;  #b is still bound to 5
>> c = 2
>> g(2) = 6 ; #c is a free parameter and resolved to 2;
>> g.__closure__[0].cell_contents  # 2
>> g.__closure__[1]   #error since only one parameter is bounded. 

b = 2 
def myh(x):
    return b*x + 1 #b is free variable, however, it always is referred to whatever global b is bound to. 

def myh2(x):
    b = 3    
    return myh(x);  #in myh, b is still referred to the global name b, not the b locally defined in myh2; thee b when myh is defined is what matters. 

>> myh2(3)   # still is 5
del b 
>> myh2(3)  #error, cannot find global name 

#+END_SRC
 
  + Variables in nested Functions are bound using /lexical scoping/. That is, names are resolved by first checking the local scope and then all enclosing scopres of the outer function definition from the innermost scope to the outermost scope. If no match is found, the global and built-in namespace are checked. Although names in enclosing scopes are accessible. Python 2 only allows variables to be reassigned in the innermost scope(local variables) and the global namespace(using global). Python 3 allows this by using nonlocal. 

  #+BEGIN_SRC python
  def countdown(n):
     def next():
       nonlocal n #only valid in python 3
       r=n
       n -= 1
       print "r=",r
     return next
  #in python 2.x , we can use list and dictionary to workaround. 
  def countdown(n):
    l=[n]
    def next():
       r=l
       r[0] -= 1
       print "r=",r
    return next
  #+END_SRC
  + When the statements that make up a function are packaged together with the environment in which they define, the resulting object is known as closure. One use of closure is for lazy/delayed evaluation. Using this closure pattern is faster than using a class to capture the variables. 
  #+BEGIN_SRC python
  from urlib.request import urlopen(Python3)
  def page(url):
      def get():
	      return urlopen(url).read()
      return get
  python = page("http://www.python.org")
  >>> python 
  <function get at 0x95d5f0>
  jython = page("http://www.jython.org")
  >>> jython
  <function get at 0x9735f0>
  # the clousre is captured in the function object 
  python.__closure__[0].cell_contents
  'http://www.python.org'
  jython.__clousre__[0].cell_contents
  'http://www.jython.org'

a = 20 
def f1():
    a = 10
    def f2():
        def f3(x):
            return (a*x)
        return f3
    return f2
>> g = f1()
>> h = g()
>> h(2) #20 
>> h.__closure__[0] ##exist 

##Note the the following doesn't form closure 
a = 20 
def f1():
    ### a = 10  #take away this line and f3() has no closure. 
    def f2():
        def f3(x):
            return (a*x)
        return f3
    return f2
>> g = f1()
>> h = g()
>> h(2) #40 
>> h.__closure__[0] ##Error. 
 
  #+END_SRC

+ Local variable may shadow global 
#+BEGIN_SRC python
x = 100
print "1. Global x:", x
class Test(object):
    y = x
    print "2. Enclosed y:", y
    x = x + 1 
    print "3. Enclosed x:", x
    z = x

    def method(self):
        print "4. Enclosed self.x", self.x
        print "5. Global x", x
        try:
            print y
        except NameError, e:
            print "6.", e

    def method_local_ref(self):
        #print x
        try:
            print x  #this cause exception caused by the line x=200 below
        except UnboundLocalError, e:
            print "7.", e
        x = 200     #this line shadows the global in the whole function which causes error in the line above.  
        print "8. Local x", x

obj = Test()
obj.method()
obj.method_local_ref()
#+END_SRC

+ Python is lexical scooping: 
The subtlety see the following examples:
Scoping in Python is dynamic and lexical. A closure will always remember the name and scope of the variable, not the object it's pointing to(http://stackoverflow.com/questions/2295290/what-do-lambda-function-closures-capture-in-python).
The following example shows the closure capture the name, not the value. So if the name later binds to different value, it affects the closure.   
#+BEGIN_SRC 

def tmp_g():
   add = range(2)
   i = 0
   print hex(id(i))  #1
   add[0] = lambda x: x+i 
   i = 1
   print hex(id(i)) #2
   add[1] = lambda x: x+i
   return add
add_rv = tmp_g() 

print add_rv[0](3) #4
print add_rv[1](3) #4

add_rv[0].__closure__[0]
<cell at xxxx: int object at #address show in #2>
add_rv[1].__closure__[1]
<cell at xxxx: int object at #address show in #2> 
#+END_SRC
* Being Pythonic
** Mutable vs. Immutable 
   For mutable types, operations that compute new values might actually return a reference to any existing object with the same type and value, for example: a = 1 and b = 1; However, this is not allowed for mutable object, for example, a = []; b= []; c = []; these empty lists are all different objects. 
** double asterisk
The double asterisk can be used in defining kwargs as well as passing a dict as arguments: 
#+BEGIN_SRC python 
args=dict(A=1, B=2);
setup(**args) ##equivalent to passing in A=1, B=2
#+END_SRC 

* Using Python for Text Parsing
** regex
#+BEGIN_SRC python
#look for the first match of number sequence(greedy). Note that re.search could start the match from any position while re.search always match from begining; Also, use +? for the nongreedy version 
m=re.search('(\d+)',"abc_2004-Q1.csv") 
m.group(1)  #2004
m=re.search('(\d+?)',"abc_2004-Q1.csv") 
m.group(1) #2
#+END_SRC 

* Performance Tricks 
** dict
+ Use /dict.intervalues()/ instead of /dict.values()/ if we only need to extract the first values
 
* Exceptions: 
#+BEGIN_SRC python 
try:
    s = dfi[col].astype(np.float64)
except:
    print "\tERROR:some info"
    raise     #re rasie the last exception

#+END_SRC
* Conda
** Info
List info including channels, configure files ect. 
conda info 
List all environments: conda info --envs 
** Create environment 
conda create -n snakes python 
or to specify a version 
conda create --name snakes python=3

** Install packages
Conda has a rule to resolve package version (http://conda.pydata.org/docs/channels.html)
We can specify the version we want to install: 
conda install pandas==0.18.1
conda install pandas>=0.15.0
** List insalled packages and versions
conda list 

* NumPy
** view vs copy 
*** simply indexing return view and advance indexing returns copy 
#+BEGIN_SRC python 
 v2=v1[1:5]
 v2[0] = 1.0 #v1[1] is also changed
 v2 is v1[1:5] # return FALSE, the internal array is the same but meta data is not which make different identity See internal data structure for detail 
#+END_SRC 
** dtype-structured array
** syntax/function compared with matlab
- numpy.where() <--> find() in matlab
#+BEGIN_SRC python1
import numpy as np
data=np.zeros((2,), dtype=[('A','i4'),('B','f4'),('C','a10')])
data[:]=[(1,2.0,'Hello'),(2,3,'World')]
#+END_SRC
* Scikit-learn 
** "Normalization" for OLS 
The normalization flag in LinearRegression constructor is not clearly explained. sklearn.linear_model.LinearRegression(normalize=True). After some digging,
we can see that this "normalize" flag is referred to rescaling the observation X. Note than "centering"(i.e, subtracting the mean) is always done regardless of this normalization flag. The source code makes use of a math property the intercept is zero after we center both Y and X. Also, the flag "normalize" is purely for preconditioning. If it set true, the coefficient/intercept are transformed back properly.  Therefore, if ill-conditioning is not an issue, "normalize" flag should not change the value of the attributes /coef_/ or /intercept_/. 

* Pandas
** Data Structure: 
*** Series
**** ndarry-like: can be passed to most NumPy function. Slicing also slice the index
**** dict-like, can set/get values by index label, like s['key']=val; However, unlike a python dict, if a label doesn't exist, an exception is raised. Note that dict-like insertion(with broadcasting) is ok with DataFrame, ie., df['new col']=val, df.loc[:,'C']=df.loc[:,'A'] can also perform enlargement (DataFrame.ix can also performs enlargement, but not .iloc). 
**** automatic label alignment 
#+BEGIN_SRC python
s[1:]+s[:-1] # due to the automatic label aligment, the first and the last entry is NaN
#+END_SRC
**** A pandas.series  can be mix-typed; this usually happen when we select a row by DataFrame.loc[label] which gives a Series of dtype: object 
#+BEGIN_SRC python
d={'C1':pd.Series([1.,2.,3.]),
   'C2':pd.Series(list('abc'))}
df=pd.DataFrame(d)
df.loc[0]
C1    1
C2    a
Name: 0, dtype: object
#+END_SRC
*** Data Frame
**** two axis: index(row labels, referred as df.index) and columns(column label,referred as df.columns)
**** can be created from a dict or Series,  numpy structured array, a list of dicts, a dict of tuples(multi-indexed)
#+BEGIN_SRC python
d={'C1':Series([1.,2.,3.,], index=['a','b','c']),
   'C2:'Series(...)}
df=DataFrame(d)
#+END_SRC
**** If the data in DataFrame is numeric, elementwise numpy function and various other numpy functions can be used with no issues.
**** There exists 3d(panel data in econometrics), 4d and nd dataframe.   
** String Manipulation
Designed for vectorization and ease to accommodate missing values. It is powerful when combined with pattern matching 
#+BEGIN_SRC python
data = {'Dave': "dave@gmail.com", 'Steve:': 'steve@gmail.com', 'Rob':np.nan}
series = pd.Series(data)
pattern = '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'
series_2 = series.str.match(pattern,flags=re.IGNORECASE)
series_2.str.get(1)
#+END_SRC 
** Index/Selection
#+BEGIN_SRC python
df['foo'] #get the columne 'foo'
df.foo #column can be accessed like class attribute if the column name is a valid python variable name.
df[s]: if s is a boolean dataframe (like df[df['col0']<0]) it selects rows instead of columns; this is a bit inconsistent but this syntax arose our of practicality; if s is a boolean dataframe(like df[df<5]] = 0), it picks up the entries.  Note that indexing always returns view instead of copy. Use Dataframe.copy() to obtain a copy   
df[df<0] #two dimension indexing; the resulting dataframe  has the same shape (still two dimensions in this cases) but the selected entries are NaN; In R, this logical indexing in two dimensions results in one dimension vector. 
df.where(df>0, -df, inplace=TRUE) # ( replace negatives with its magnitudes. 
df.mask(df>0) # replace the positive entries with NaN
df.lookup([row1,row2],[col1,col2]) #return 1d numpy array with two entries (row1,col1),(row2,col2))
df.select(lambda x: x=='A', axis=1) #keep the column 'A' only 
df.isin({'A':[1,3],'B':[2,4]})
df[df.A<10 & df.B>10]
df.query('colA == "Hello" & colB == "World"')
#+END_SRC
*** Query
- A very succinct and convenient syntax 
#+BEGIN_SRC python
df.query('b==["a","b","c"]') #select rows where column b is the one of the specified values. 
df.query('b in a') #select rows where column b is one of the values in column a; which is equivalent to  df[df.b.isin(df.a)]
#+END_SRC
- A bug makes it extremely dangerous, 
#+BEGIN_SRC python
df.query("A='abc'")  # This reset the columan A! '=' should be '=='
#+END_SRC
*** Three indexing methods: by label (.loc), by position(.iloc)  and advance indexing (.ix) 
#+BEGIN_SRC python
df.loc[row_idx, col_idx]
panel.loc[item_idx, major_idx, minor_idx) #assume all for unspecified dim. 
df[colnames] #([] correponds __getitem__, selecting low-dim slice 
panel[itemnames]  
#+END_SRC

*** use at()/iat() methods for fast scalar value getting and setting([] is slow since it must handle a lot of cases) 
*** Gotchas 
+ df.loc[start_idx:last_idx] includes the last index but df[start_idx:last_idx] doesn't. 
** MultiIndex(column/row)
*** Select rows with list of values in a level 
#+BEGIN_SRC python 
gb = df[['date','seg','balance']].groupby(['date','seg']).agg('sum')
gb[gb.index.get_level_values("seg").isin(["A","B"])]
#+END_SRC
*** Swaplevel and sortlevel
#+BEGIN_SRC python
gb.swaplevel("seg","date").sortlevel("seg")
#+END_SRC
*** flatten multi-index(rows) 
#+BEGIN_SRC python 
df = pd.DataFrame({'key':['a','a','b','b'], 'val1':np.arange(4), 'val2':np.arange(4)})
gb = df.groupby('key')
In [11]: gba = gb.agg({'val1':'size', 'val2':[np.mean,np.sum]})
Out[11]: 
    val2     val1
    mean sum size
key              
a    0.5   1    2
b    2.5   5    2
In [17]: gba.columns.values
Out[17]: array([('val2', 'mean'), ('val2', 'sum'), ('val1', 'size')], dtype=object)

In [18]: gba.columns. get_level_values(0)
Out[18]: Index([u'val2', u'val2', u'val1'], dtype='object')

In [19]: gba.columns. get_level_values(1)
Out[19]: Index([u'mean', u'sum', u'size'], dtype='object')

In [20]: gba.columns = gba.columns.get_level_values(0)

In [21]: gba
Out[21]: 
     val2  val2  val1
key                  
a     0.5     1     2
b     2.5     5     2

In [22]: gba['val2']
Out[22]: 
     val2  val2
key            
a     0.5     1
b     2.5     5


#+END_SRC
*** Turn on/off sparse hierarchical output
#+BEGIN_SRC python
pd.set_option('display.multi_sparse',True)
#+END_SRC
*** pandas index/multi-index values may not not uniques

#+BEGIN_SRC python
df = pd.DataFrame(np.ones((3,3)), index =['A','B','C'], columns=['foo_1','foo_2', 'foo_3']).set_index(['foo_1','foo_2']).loc[(1,1),:] # this is valid and will return everything. 


#+END_SRC
** Insert/delete rows/columns
df.append(dict-like) # no check for new key; so spelling error would create a new column ; also not in place. 
df['newcol'] = series 
del df['col']
** Chaining 
#+BEGIN_SRC python
iris=read_csv('data/iris.data')
iris.query('SepalLength > 5').assign(SepalRatio=lambda x: x.SepalWidth/x.SepalLength, PetalRatio=lambda x: x.PetalWidth/PetalLength).plot(kind='scatter', x='SepalRatio',y='PetalRatio') 
#+END_SRC
** Dataframe: apply
#+BEGIN_SRC python
   df.apply(lambda x,y: x.where(x>0,y), y=df['A']) #same as df.where(df>0,df['A'],axis='index',inplace=TRUE))
#+END_SRC
** display
#+BEGIN_SRC python
> set_option('display.width',160) #default is 80
> df.head()
> bassball=read_csv('data/baseball.csv')
> baseball.info()
> baseball.ilocal[-20:, :12].to_string()
#+END_SRC
** exploratory data analysis
#+BEGIN_SRC python
df.info (): summary 
s.value_count()
df.dtypes  #show the dtypes for each column
df.columns=[x.lower() for x in df.columns] #show column names in lower case; 
df.values #access the values #for heterogeneous data, dtype will be chosen to accommodate all data invloved.
df.get_dtype_counts() #return the number of columns of each type
df.someColumn.str.lower #vectorized string methods are available through string attribute. Note that for the column with string, the dtype is object 
df.convert_objects(convert_numeric=True) convert string that represents a numbedf.select_dtypes(include=['bool','int64']) #select the column of dtype bool and int64
df.someCol.isin([2,4,6]) #return boolean vectors to indicate whether someCol in {2,4,6}; this boolean vectors can furthur be used for slicing.
#+END_SRC
** Iterator 
#+BEGIN_SRC python 
for idx, row in df.iterrows(): 
     print ('%s\n%s' % (idx,row))
#row is a reference, change to row will also alter df. 
#+END_SRC

** Accelerated operations
Pandas has support accelerating certain types of binary numerical and boolean operations using the numexpr and bottleneck libraries. 

** convenient features
Pandas has the following convenient features 
+ combining two overlapping dataset( A is more preferred but B has more data)
 
** Time Series
#+BEGIN_SRC python
from dateutil.relativedelta import relativedelta

#+END_SRC
** Gotcha
*** Never specify the dtype when reading in data. Instead, first read in, then convert http://stackoverflow.com/questions/15210962/specifying-dtype-with-pandas-read-csv
#+BEGIN_SRC python
# dont' use dtype converters explicity for the columns you care about
# they will be converted to float64 if possible, or object if they cannot
df = pd.read_csv('test.csv'.....)

#### this is optional and related to the issue you posted ####
# force anything that is not a numeric to nan
# columns are the list of columns that you are interesetd in
df[columns] = df[columns].convert_objects(convert_numeric=True)


    # astype
    df[columns] = df[columns].astype('float32')

see http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion

Its not as efficient as doing it directly in read_csv (but that requires
#+END_SRC

*** Merge on columns with NaN.
NaN will NOT be ignored during the merge, instead it will merge if the other dataframe also has a NaN. Exclude the NaN from the other data frame is not desired.
#+BEGIN_SRC python 
In [23]: df = pd.DataFrame(data= np.arange(6).reshape((3,2)),columns=['id','val'])

In [24]: df
Out[24]: 
   id  val
0   0    1
1   2    3
2   4    5

In [25]: df.iloc[2,0] = np.nan

In [26]: df
Out[26]: 
    id  val
0  0.0    1
1  2.0    3
2  NaN    5

In [27]: df2 = pd.DataFrame(data= np.arange(6).reshape((3,2)),columns=['id','val2'])

In [28]: df2.iloc[2,0] = np.nan

In [29]: df2
Out[29]: 
    id  val2
0  0.0     1
1  2.0     3
2  NaN     5

In [30]: df.merge(df2, on ='id', how = 'left')
Out[30]: 
    id  val  val2
0  0.0    1     1
1  2.0    3     3
2  NaN    5     5

In [31]: df.merge(df2[df2.id.notnull()], on ='id', how = 'left')
Out[31]: 
    id  val  val2
0  0.0    1   1.0
1  2.0    3   3.0
2  NaN    5   NaN

#+END_SRC
*** Merge on float vs. int. 
if df1.col is float type and df2.col in int type, the merge is carried out based on value equality. 
#+BEGIN_SRC python 
df1 = pd.DataFrame(data= np.arange(6).reshape((3,2)),columns=['id','val'])
df2 = pd.DataFrame(data= np.arange(6).reshape((3,2)),columns=['id','val'])
df1.iloc[2,0] = np.nan
df1.iloc[0,0] = 1e-10
In [43]: df1
Out[43]: 
             id  val
0  1.000000e-10    1
1  2.000000e+00    3
2           NaN    5

In [44]: df1.merge(df2, on = 'id', how='left'
    ...: )
Out[44]: 
             id  val_x  val_y
0  1.000000e-10      1    NaN
1  2.000000e+00      3    3.0
2           NaN      5    NaN

#+END_SRC
* Pymssql 
** Doc: http://www.pymssql.org/en/latest/intro.html
** The parameter substitution of pymssql works sliently different than the python DB-API documented in https://code.google.com/p/pyodbc/wiki/Cursor. (Cf.http://stackoverflow.com/questions/23244450/cant-insert-tuple-to-mssql-db). 
#+BEGIN_SRC python
cursor.execute("select a from tbl where b=? and c=?", (10, 'Hello')) #Error; should change to: 
cursor.execute("select a from tbl where b=%d and c=%s", (10, 'Hello')) # string will be properly quoted. 
#+END_SRC
* matplotlib
If X-window is not setup correctly and matplotlib pylab is not working properly, try:
** Use inline
		%matplotlib inline
		pylab,plot(x,y)
		pylab.show()
** Use non-interactive backend:   
   import matplotlib
   matplotlib.use('Agg')  # this should put before pylab.plot()
   pylab.plot(x,y)
   pylab.savefig("test.png")
