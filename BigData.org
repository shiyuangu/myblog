* Spark
** RDD: 
+ Action vs. Transformation 
+ Persistence, Storage level 
each time we call a new action, the entire RDD must be computed "from scratch". To avoid this inefficiency, users can /persist/ immediate results. 
** Programming Framework 
http://spark.apache.org/docs/latest/programming-guide.html
*** Session vs. Context 
** terminology 
Slice  = Partition 
** Text File: 
+ By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.
+ Q: How to know the record to have keys or not? 
TODO: read more about InputFormat in general 
#+END_SRC 
* Spark SQL
** RDD vs. Dataset vs. DataFrame
http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes
DataFrame are RDD of Row 
#+BEGIN_SRC  python 
In [2]: df = spark.createDataFrame([
   ...:     (0, "a b c".split(" ")),
   ...:     (1, "a b b c a".split(" "))
   ...: ], ["id", "words"])
In [16]: df.rdd.collect()
Out[16]: 
[Row(id=0, words=[u'a', u'b', u'c']),
 Row(id=1, words=[u'a', u'b', u'b', u'c', u'a'])]
#+END_SRC

**  Dataframe vs. Pandas dataframe
http://www.kdnuggets.com/2016/01/python-data-science-pandas-spark-dataframe-differences.html
Spark dataframe is /immutatble/ and is designed for lazy evaluation, distributed computation. 

** Create DataFrame
pyspark.sql.dataframe.DataFrame can be create directly from text, json, ect. Alternatively, it can also be created from a RDD. The schema can be inferred automatically or be supplied by StructType(which is a list of StructField). DataFrame.rdd returns a RDD for that dataframe(after spark 2.0). 
*** create DataFrame from RDD
#+BEGIN_SRC python 
    data_path = "r52-no-stop.txt" 
    spark = SparkSession\
        .builder\
        .appName("TfIdfExample")\
        .getOrCreate()

    #each line is a document; each document is represented by
    # a "word" for the class label followed by a TAB and then a seq of words
    # delimited by spaces
    sc = spark.sparkContext 
    documents = sc.textFile(data_path)\
                  .map(lambda l: l.split("\t"))\
                  .map(lambda p: Row(label=p[0],sentence=p[1]))

    documents = spark.createDataFrame(documents)
#+END_SRC

*** useful functions 
#+BEGIN_SRC python 
from pyspark.sql import SparkSession
from pyspark.sql import Row
spark = SparkSession.builder.appName("PythonSQL").config("spark.some.config.option","some-value").getOrCreate()
lines = sc.textFile("people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name = p[0], age= int(p[1])))
schemaPeople = spark.createDataFrame(people)
type(schemaPeople)  #Dataframe
schemaPeople.show(truncate=False)
schemaPeople.createOrReplaceTempView("people") #so that we can query the table through the SQL as follows. 
teenagers= spark.sql("SELECT name FROME people where age >=13 and age <=19")
teenagers= spark.sql("SELECT name FROM people where age >=13 and age <=19")
type(teenagers) #Dataframe
teenNames = teenagers.rdd.map(lambda: "Name: " + p.name)  #teenNames is a RDD

** Schema 
*** print 
#+BEGIN_SRC python 
from pyspark.ml.feature import HashingTF, Tokenizer
training = spark.createDataFrame([
    (0L, "a b c d e spark", 1.0),
    (1L, "b d", 0.0),
    (2L, "spark f g h", 1.0),
    (3L, "hadoop mapreduce", 0.0)], ["id", "text", "label"])
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
rv = tokenizer.fit(training)
rv2 = hashingTF.transform(rv)
In [92]: type(rv2)
Out[92]: pyspark.sql.dataframe.DataFrame
In [90]: rv2.printSchema()
root
 |-- id: long (nullable = true)
 |-- text: string (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- features: vector (nullable = true)

#+END_SRC
** metadata: 
http://stackoverflow.com/questions/32628845/is-there-a-way-to-add-extra-metadata-for-spark-dataframes
columns in dataframe can have metadata 
#+BEGIN_SRC python
In [18]: df = spark.createDataFrame(
    ...:     [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c
    ...: ")],
    ...:     ["id", "category"])
    ...: 

In [19]: indexer = StringIndexer(inputCol="category", outputCol="categ
    ...: oryIndex")

In [20]: model = indexer.fit(df)
    ...: indexed = model.transform(df)
    ...: 

In [21]: indexed.show()
    ...: 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+

In [23]: indexed.schema.fields
Out[23]: 
[StructField(id,LongType,true),
 StructField(category,StringType,true),
 StructField(categoryIndex,DoubleType,true)]

In [26]: indexed.printSchema()
root
 |-- id: long (nullable = true)
 |-- category: string (nullable = true)
 |-- categoryIndex: double (nullable = true)


In [24]: [t.metadata for t in indexed.schema.fields]
Out[24]: 
[{},
 {},
 {u'ml_attr': {u'name': u'categoryIndex',
   u'type': u'nominal',
   u'vals': [u'a', u'c', u'b']}}]


#+END_SRC
** Chain operation 
Note the use of the df.select(..).withColumn(..) and col(""). About the use of col("exp") function: not needed if exp is just a column name, but needed if more complicated expression (Cf. https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/sql/Column.html) 
#+BEGIN_SRC python 
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
sentenceDataFrame = spark.createDataFrame([
    (0, "Hi I heard about Spark"),
    (1, "I wish Java could use case classes"),
    (2, "Logistic,regression,models,are,neat")
], ["id", "sentence"])
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
regexTokenizer = RegexTokenizer(inputCol="sentence", outputCol="words", pattern="\\W")
# alternatively, pattern="\\w+", gaps(False)
countTokens = udf(lambda words: len(words), IntegerType())
tokenized = tokenizer.transform(sentenceDataFrame)
tokenized.select("sentence", "words")\
    .withColumn("tokens", countTokens(col("words"))).show(truncate=False)
regexTokenized = regexTokenizer.transform(sentenceDataFrame)
regexTokenized.select("sentence", "words") \
    .withColumn("tokens", countTokens(col("words"))).show(truncate=False)

#+END_SRC

** common functions: 
#+BEGIN_SRC python 
# number of rows, 
df.count() #return number of row regardless NA or not(vs. Pandas) 

# number of columns
len(df.columns)

# show the first row  
df.first()

#only take the first n row; 
#df.limit() return a new dataframe while head()/take() return a list of Row
df.limit(1).collect() [Row(age=2, name=u'Alice')]

# show unique values of a column 
df.select('col_name').distinct().show()

#split a dataset 
df_train, df_test = df.randomSplit([0.8,0.2])

#Stratified sampling: 
>>> from pyspark.sql.functions import col 
>>> dataset = sqlContext.range(0, 100).select((col("id") % 3).alias("key")) 
>>> sampled = dataset.sampleBy("key", fractions={0: 0.1, 1: 0.2}, seed=0) 
>>> sampled.groupBy("key").count().orderBy("key").show() 

#convert  row of vectors to np.array, /d/ below is a Row
w2v_plot = np.array(w2v.rdd.map(lambda d: d.vector.toArray()).take(100))

#+END_SRC
* Spark MLlib
** Pipeline
*** Summary
+ inspired by /sklearn/, and relied on Dataframe.  
+ Transformers, Estimators, Evaluator
Transformer.transform()s and Estimator.fit()s are both stateless. /Model/ is a transformer.  
+ Pipeline can be nonlinear (DAG) 
+ Same instance cannot be inserted into the pipeline twice(but can be two instance of the same type)
+ PipelineModel consists of multiple transformers and estimators(Cf. Pipeline.scala) 
#+BEGIN_SRC python
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

# Prepare training documents from a list of (id, text, label) tuples.
training = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0)
], ["id", "text", "label"])

# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# Fit the pipeline to training documents.
model = pipeline.fit(training)

# Prepare test documents, which are unlabeled (id, text) tuples.
test = spark.createDataFrame([
    (4, "spark i j k"),
    (5, "l m n"),
    (6, "spark hadoop spark"),
    (7, "apache hadoop")
], ["id", "text"])

# Make predictions on test documents and print columns of interest.
prediction = model.transform(test)
selected = prediction.select("id", "text", "probability", "prediction")
for row in selected.collect():
    rid, text, prob, prediction = row
    print("(%d, %s) --> prob=%s, prediction=%f" % (rid, text, str(prob), prediction))
#+END_SRC
*** vs. Pipeline in sklearn 
sklearn Pipeline consists of both transformers and estimators. Transformers in sklearn implement both /fit/ and /transfrom/ method while estimators in sklearn implements /fit/ and /predict/ methods. Also, transformers/estimators in sklearn might has states while /transformers/ and /estimators/ in Spark are *stateless*. /estimators/ in sklearn rely on its internal state for prediction while /estimators/ in spark returns a /model/(which is a transformer)
Cf. [[https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/pipeline.py#L86][Pipeline:_validate_steps]]
Cf. [[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html][FunctionTransformer]]
Cf. [[http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator][rolling your own estimator]]

*** Param and ParamMap
+ MLlib /Estimators/ and /Transformer/s use a uniform API for specifying parameters.
+ Parameters belong to specific instances of Estimators and Transformers. For example, if we have two LogisticRegression instances lr1 and lr2, then we can build a ParamMap with both maxIter parameters specified: ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20).
#+BEGIN_SRC 
# to see the parameters for an estimator 
In [33]: lr = LogisticRegression(maxIter=10, regParam=0.01)
In [34]: print("LogisticRegression parameters:\n" + lr.explainParams()
    ...:  + "\n")
In [39]: lr.maxIter
Out[39]: Param(parent=u'LogisticRegression_4f8190d0025ad21226c0', name='maxIter', doc='max number of iterations (>= 0).')

n [41]: paramMap = {lr.maxIter: 20}
In [42]: paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) 
In [43]: paramMap2 = {lr.probabilityCol: "myProbability"}
In [44]: paramMapCombined = paramMap.copy()
In [45]: paramMapCombined.update(paramMap2)
In [46]: model2 = lr.fit(training, paramMapCombined)

# extractParamMap() is not working in PySpark https://issues.apache.org/jira/browse/SPARK-13368
print(model2.extractParamMap()) 
#+END_SRC

*** Cross Validation 
After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset.
Q: how to extract the best model?
A: /cvModel.bestModel/; if /cvModel/ is a Pipeline model, each stage can be retrieved by /cvModel.bestMode.stages[i]/ 
Q: how to extract the best parameters from cvModel
A: look at: /cvModel.getEstimatorParamMaps()/ and /cvModel.avgMetrics/ 
#+BEGIN_SRC python 
#print the metrics for each combination of parameters in cv search 
paramNames_pp=[";".join(["%s:%g" % (k.name,v) for k,v in p.items()]) for p in cvModel.getEstimatorParamMaps()]
print(zip(paramNames_pp, cvModel.avgMetrics))
#+END_SRC

** Vector and Array 
#+BEGIN_SRC python 
In [94]: v1
Out[94]: DenseVector([0.0, 1.1, 0.1])
In [95]: v2 = 
KeyboardInterrupt
In [95]: v2 = Vectors.dense([0.0, 1.1, -.1])
In [96]: rv = sc.parallelize([v1,v2])
In [97]: rv
Out[97]: ParallelCollectionRDD[149] at parallelize at PythonRDD.scala:475
#+END_SRC
*** Sparse Vector and Sparse matrix 
#+BEGIN_SRC python 
from pyspark.mllib.linalg import Vectors
import scipy.sparse as sps 
sv1 = Vectors.sparse(3,[0,2],[1.0,3.0]) # the first integer is the size
sv2 = sps.csc_matrix(..)
#+END_SRC
SVD 
#+BEGIN_SRC scala
//From the book of Learning Spark 
val svd = mat.computeSVD(3,computeU=true) //the type of svd is SingularValueDecomposition[RowMatrix,Matrix] 
val s: Vector = svd.s
val V: Matrix = svd.V #V is a local matrix.for the first k svd components, V is of size n by k 
#+END_SRC

*** Matrix multiplcation 
Rf. /Learning Spark/
+ Local vector/matrix depends on the API. for Python, we can usually directly pass numpy array or use the /pyspark.mllib.linalg.Vectors/ which is a wrapper round numpy. In particular, we can get the numpy array from pyspark array using the /toArray/ method ; for Scala/Java, it's /org.apache.spark.mllib.linalg.Vectors/ which are wrappers around /Breeze/ array. spark mllib around are primarily meant for data representation and do not provide arithmetic operations such as addition and subtraction. 
+ Distributed matrices are wrappers around RDD[]. 
+ Note that the indices of /IndexedRowMatrix/ might not be consecutive(http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix). When a /IndexedRowMatrix/ multiplies a local matrix, it converts to /RowMatrix/ and then zip back the indices. 
#+BEGIN_SRC scala
scala> val idx = "^v([0-9]+)$".r
idx: scala.util.matching.Regex = ^v([0-9]+)$
scala> val rdd = sc.parallelize(List(
      (("v1","v1"),2),(("v1","v2"),4),  
      (("v1","v3"),1),(("v2","v2"),5),  
      (("v2","v3"),1),(("v3","v3"),2)   
      ))                                
scala> val mat = new CoordinateMatrix(rdd.map {case ((idx(i),idx(j)),v) =>
      MatrixEntry(i.toLong-1,j.toLong-1,v.toDouble) 
      }).toIndexedRowMatrix                         
                                                  

//we can multiply a distributed matrix by a local matrix(only in Scala API, not Python)
val vector = Matrices.dense(3,1,Array(4.0,1.0,5.0))
mat.multiply(vector).rows.map(println).collect()

//or we can do it in map-reduce way 
val vec = sc.parallelize(List(("v1",4),("v2",1),("v3",5)))
val t = (rdd.map {case ((i,j),v) => (j,(i,v))} join vec).values map {case ((i,v1),v2) => (i,v1*v2)} reduceByKey(_+_)

//If we want to multiply two distributed matrices, both of them needs to be block matrix. 


#+END_SRC
** Feature Engineering 
*** Tokenizer 
*** TF: 
+ Hashing TF:
Why number of buckets is advisable to be 2^k?
To read: https://en.wikipedia.org/wiki/Feature_hashing
+ ConutVectorizer:
/CountVectorizer/ is an /estimator/. During the fitting process, CountVectorizer will select the top /vocabSize/ words ordered by term frequency across the corpus. CountVectorizer.fit() generates a /CountVectorizerModel/. /CountVectorizerModel::transform(df_test)/ counts the term frequency in /df_test/ according to the /vocabulary/; the words not in the /vocabulary/ are ignored.  
#+BEGIN_SRC python
In [2]: df = spark.createDataFrame([
   ...:     (0, "a b c".split(" ")),
   ...:     (1, "a b b c a".split(" "))
   ...: ], ["id", "words"])

In [3]: cv = CountVectorizer(inputCol="words", outputCol="features", vocabSize=3, minDF=2.0)

In [4]: model = cv.fit(df)

In [5]: model.vocabulary
Out[5]: [u'b', u'a', u'c']

In [9]: df_test = spark.createDataFrame([(0, "c c c d a".split(" "))],["id","words"])

In [10]: df_test.show()
+---+---------------+
| id|          words|
+---+---------------+
|  0|[c, c, c, d, a]|
+---+---------------+


In [11]: model.transform(df)
Out[11]: DataFrame[id: bigint, words: array<string>, features: vector]

In [12]: model.transform(df).show(truncate=True)
+---+---------------+--------------------+
| id|          words|            features|
+---+---------------+--------------------+
|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|
|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|
+---+---------------+--------------------+


In [13]: model.transform(df).show(truncate=False)
+---+---------------+-------------------------+
|id |words          |features                 |
+---+---------------+-------------------------+
|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|
|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|
+---+---------------+-------------------------+


In [14]: model.transform(df_test).show(truncate=False)
+---+---------------+-------------------+
|id |words          |features           |
+---+---------------+-------------------+
|0  |[c, c, c, d, a]|(3,[1,2],[1.0,3.0])|
+---+---------------+-------------------+

#+END_SRC
*** IDF: 
Note that IDF doesn't normal the vector(vs. TfidfVectorizer in sklearn)
*** Word2Vec
The tutorial is misleading regarding to this example. sample_lda_data.txt is the training data for this example, instead of text8 data(even though this is the standard training set for word2vec). http://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/feature.html#Word2Vec
#+BEGIN_SRC python
from pyspark.mllib.feature import Word2Vec

#sgu: 
inp = sc.textFile("data/mllib/sample_lda_data.txt").map(lambda row: row.split(" "))

word2vec = Word2Vec()
model = word2vec.fit(inp)

synonyms = model.findSynonyms('1', 5)

for word, cosine_distance in synonyms:
    print("{}: {}".format(word, cosine_distance))
#+END_SRC
*** StringIndexer, Index2String  
+ StringIndexer can be used to turn string into numeric index. As Spark 2.1.0, classifiers cannot handle string label so StringIndexer is needed  
#+BEGIN_SRC python 
In [13]: df = spark.createDataFrame(
    ...:     [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c
    ...: ")],
    ...:     ["id", "category"])
    ...: 

In [14]: indexer = StringIndexer(inputCol="category", outputCol="categ
    ...: oryIndex")

In [15]: indexed = indexer.fit(df).transform(df)

In [16]: indexed.show()
    ...: 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+


In [17]: indexed.printSchema()
root
 |-- id: long (nullable = true)
 |-- category: string (nullable = true)
 |-- categoryIndex: double (nullable = true)

#+END_SRC

+ Q: how to map the index back to string? 
We can use the meta data in the column created by StringIndexer to convert numeric column back to string. 
#+BEGIN_SRC python 
    #Map back the predict index to label name if needed 
    df, df_pred = documents, df_train 
    name_mapping = dict([(col,t.metadata) for col, t in zip(df.columns,df.schema.fields)])['label_n']['ml_attr']['vals']
    df_pred = IndexToString(inputCol='label_pred_n',outputCol='label_pred',labels = name_mapping).transform(df_pred)
#+END_SRC

** text segmentation
Stanford NLP Group, scalanlp/chalk 
*** Word2Vec
Estimator also. 
http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec
** Classification
*** What's the rawPrediction as in LogisticRegression ctor?
For LogisticRegression, it's the logit (the linear function value, equiv. to  /prediction(glm_fit,new_data type="link") in R/ 
http://stackoverflow.com/questions/37903288/what-do-colum-rawprediction-and-probability-of-dataframe-mean-in-spark-mllib
*** What's aggregationDepth param in LogisticRegression ctor? 
This is related to an improvement of MLlib regarding to MapReduce Paradigm. Multi-level aggregation tree is used to combined partially on a small set of executors before they sent to the drivers which is dramatically reduces the load of the driver. https://databricks.com/blog/2014/09/22/spark-1-1-mllib-performance-improvements.html
Note this WIP: https://issues.apache.org/jira/browse/SPARK-17090
