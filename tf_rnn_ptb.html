<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Tensorflow RNN PTB Example Walkthrough</title>
<!-- 2017-01-17 Tue 00:12 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="sgu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Tensorflow RNN PTB Example Walkthrough</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Structure</a></li>
<li><a href="#sec-2">2. Building the Graph</a></li>
<li><a href="#sec-3">3. Feeding the States of LSTM</a></li>
</ul>
</div>
</div>
<hr  />
<hr  />
<p>
This is a walkthrough of the tensorflow example here
</p>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Structure</h2>
<div class="outline-text-2" id="text-1">
<p>
There is only one graph, but there are three PTBModel instances to keep track of the elements related to the stage of train, valid and test. Note the use of <code>variable_scope</code> and the <code>resue</code> to share the weights/bias. 
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #a020f0;">with</span> tf.Graph().as_default():
  <span style="color: #000000; background-color: #ffffff;">initializer</span> = tf.random_uniform_initializer(-config.init_scale,
                                              config.init_scale)

  <span style="color: #a020f0;">with</span> tf.name_scope(<span style="color: #8b2252;">"Train"</span>):
    <span style="color: #000000; background-color: #ffffff;">train_input</span> = PTBInput(<span style="color: #000000; background-color: #ffffff;">config</span>=config, <span style="color: #000000; background-color: #ffffff;">data</span>=train_data, <span style="color: #000000; background-color: #ffffff;">name</span>=<span style="color: #8b2252;">"TrainInput"</span>)
    <span style="color: #a020f0;">with</span> tf.variable_scope(<span style="color: #8b2252;">"Model"</span>, <span style="color: #000000; background-color: #ffffff;">reuse</span>=<span style="color: #a020f0;">None</span>, <span style="color: #000000; background-color: #ffffff;">initializer</span>=initializer):
      <span style="color: #000000; background-color: #ffffff;">m</span> = PTBModel(<span style="color: #000000; background-color: #ffffff;">is_training</span>=<span style="color: #a020f0;">True</span>, <span style="color: #000000; background-color: #ffffff;">config</span>=config, <span style="color: #000000; background-color: #ffffff;">input_</span>=train_input)
    tf.scalar_summary(<span style="color: #8b2252;">"Training Loss"</span>, m.cost)
    tf.scalar_summary(<span style="color: #8b2252;">"Learning Rate"</span>, m.lr)

  <span style="color: #a020f0;">with</span> tf.name_scope(<span style="color: #8b2252;">"Valid"</span>):
    <span style="color: #000000; background-color: #ffffff;">valid_input</span> = PTBInput(<span style="color: #000000; background-color: #ffffff;">config</span>=config, <span style="color: #000000; background-color: #ffffff;">data</span>=valid_data, <span style="color: #000000; background-color: #ffffff;">name</span>=<span style="color: #8b2252;">"ValidInput"</span>)
    <span style="color: #a020f0;">with</span> tf.variable_scope(<span style="color: #8b2252;">"Model"</span>, <span style="color: #000000; background-color: #ffffff;">reuse</span>=<span style="color: #a020f0;">True</span>, <span style="color: #000000; background-color: #ffffff;">initializer</span>=initializer):
      <span style="color: #000000; background-color: #ffffff;">mvalid</span> = PTBModel(<span style="color: #000000; background-color: #ffffff;">is_training</span>=<span style="color: #a020f0;">False</span>, <span style="color: #000000; background-color: #ffffff;">config</span>=config, <span style="color: #000000; background-color: #ffffff;">input_</span>=valid_input)
    tf.scalar_summary(<span style="color: #8b2252;">"Validation Loss"</span>, mvalid.cost)

  <span style="color: #a020f0;">with</span> tf.name_scope(<span style="color: #8b2252;">"Test"</span>):
    <span style="color: #000000; background-color: #ffffff;">test_input</span> = PTBInput(<span style="color: #000000; background-color: #ffffff;">config</span>=eval_config, <span style="color: #000000; background-color: #ffffff;">data</span>=test_data, <span style="color: #000000; background-color: #ffffff;">name</span>=<span style="color: #8b2252;">"TestInput"</span>)
    <span style="color: #a020f0;">with</span> tf.variable_scope(<span style="color: #8b2252;">"Model"</span>, <span style="color: #000000; background-color: #ffffff;">reuse</span>=<span style="color: #a020f0;">True</span>, <span style="color: #000000; background-color: #ffffff;">initializer</span>=initializer):
      <span style="color: #000000; background-color: #ffffff;">mtest</span> = PTBModel(<span style="color: #000000; background-color: #ffffff;">is_training</span>=<span style="color: #a020f0;">False</span>, <span style="color: #000000; background-color: #ffffff;">config</span>=eval_config,
                      <span style="color: #000000; background-color: #ffffff;">input_</span>=test_input)
</pre>
</div>
<p>
A typical epoch loop follows. Each epoch iteration completes a full pass of the train, valid and test dataset. Note that the learning rate are kept constant in each epoch and only updated across epoch. 
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #000000; background-color: #ffffff;">sv</span> = tf.train.Supervisor(<span style="color: #000000; background-color: #ffffff;">logdir</span>=FLAGS.save_path)
<span style="color: #a020f0;">with</span> sv.managed_session() <span style="color: #a020f0;">as</span> session:
  <span style="color: #a020f0;">for</span> i <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(config.max_max_epoch):
    <span style="color: #000000; background-color: #ffffff;">lr_decay</span> = config.lr_decay ** <span style="color: #483d8b;">max</span>(i + <span style="color: #000000; background-color: #ffffff;">1</span> - config.max_epoch, <span style="color: #000000; background-color: #ffffff;">0</span>.<span style="color: #000000; background-color: #ffffff;">0</span>)
    m.assign_lr(session, config.learning_rate * lr_decay)

    <span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">"Epoch: %d Learning rate: %.3f"</span> % (i + <span style="color: #000000; background-color: #ffffff;">1</span>, session.run(m.lr)))
    <span style="color: #000000; background-color: #ffffff;">train_perplexity</span> = run_epoch(session, m, <span style="color: #000000; background-color: #ffffff;">eval_op</span>=m.train_op,
                                 <span style="color: #000000; background-color: #ffffff;">verbose</span>=<span style="color: #a020f0;">True</span>)
    <span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">"Epoch: %d Train Perplexity: %.3f"</span> % (i + <span style="color: #000000; background-color: #ffffff;">1</span>, train_perplexity))
    <span style="color: #000000; background-color: #ffffff;">valid_perplexity</span> = run_epoch(session, mvalid)
    <span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">"Epoch: %d Valid Perplexity: %.3f"</span> % (i + <span style="color: #000000; background-color: #ffffff;">1</span>, valid_perplexity))

  <span style="color: #000000; background-color: #ffffff;">test_perplexity</span> = run_epoch(session, mtest)
  <span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">"Test Perplexity: %.3f"</span> % test_perplexity)

  <span style="color: #a020f0;">if</span> FLAGS.save_path:
    <span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">"Saving model to %s."</span> % FLAGS.save_path)
    sv.saver.save(session, FLAGS.save_path, <span style="color: #000000; background-color: #ffffff;">global_step</span>=sv.global_step)
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Building the Graph</h2>
<div class="outline-text-2" id="text-2">
<p>
RNN is for sequence model. Each learning example is a pair of (<code>input_data</code>, <code>target</code>) where <code>input_data</code> and <code>target</code> are both sequence of the same length. Moreover, <code>input_data</code> and <code>target</code> are only differed by 1  in position.  For example, Given a sequence <code>(w_1,w_2,....)</code>, if <code>input</code> is <code>(w_1,w_2,..,w_n)</code>, the target is <code>(w_2,w_3, ..., w_n)</code>.  Also, note that how RNN is unrolled. I have commented the parts worth attention.  
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #a020f0;">class</span> <span style="color: #228b22;">PTBModel</span>(object):
  <span style="color: #8b2252;">"""The PTB model."""</span>

  <span style="color: #a020f0;">def</span> <span style="color: #0000ff;">__init__</span>(<span style="color: #a020f0;">self</span>, is_training, config, input_):
    <span style="color: #a020f0;">self</span>._input = input_

    <span style="color: #000000; background-color: #ffffff;">batch_size</span> = input_.batch_size  <span style="color: #b22222;">#20</span>
    <span style="color: #000000; background-color: #ffffff;">num_steps</span> = input_.num_steps <span style="color: #b22222;">#20, the length of the sequence in each learning example. </span>
    <span style="color: #000000; background-color: #ffffff;">size</span> = config.hidden_size <span style="color: #b22222;">#200-1500 depends on the config</span>
    <span style="color: #000000; background-color: #ffffff;">vocab_size</span> = config.vocab_size

    <span style="color: #b22222;"># Slightly better results can be obtained with forget gate biases</span>
    <span style="color: #b22222;"># initialized to 1 but the hyperparameters of the model would need to be</span>
    <span style="color: #b22222;"># different than reported in the paper.</span>
    <span style="color: #000000; background-color: #ffffff;">lstm_cell</span> = tf.nn.rnn_cell.BasicLSTMCell(size, <span style="color: #000000; background-color: #ffffff;">forget_bias</span>=<span style="color: #000000; background-color: #ffffff;">0</span>.<span style="color: #000000; background-color: #ffffff;">0</span>, <span style="color: #000000; background-color: #ffffff;">state_is_tuple</span>=<span style="color: #a020f0;">True</span>) <span style="color: #b22222;">#sgu: This line doesn't create the weights/bias insight the LSTM cell, not yet.</span>

    <span style="color: #b22222;">#sgu: Dropout is applied for medium/large configuration</span>
    <span style="color: #a020f0;">if</span> is_training <span style="color: #a020f0;">and</span> config.keep_prob &lt; <span style="color: #000000; background-color: #ffffff;">1</span>:
      <span style="color: #000000; background-color: #ffffff;">lstm_cell</span> = tf.nn.rnn_cell.DropoutWrapper(
          lstm_cell, <span style="color: #000000; background-color: #ffffff;">output_keep_prob</span>=config.keep_prob)
    <span style="color: #000000; background-color: #ffffff;">cell</span> = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers, <span style="color: #000000; background-color: #ffffff;">state_is_tuple</span>=<span style="color: #a020f0;">True</span>)

    <span style="color: #a020f0;">self</span>._initial_state = cell.zero_state(batch_size, data_type())

    <span style="color: #a020f0;">with</span> tf.device(<span style="color: #8b2252;">"/cpu:0"</span>):
      <span style="color: #b22222;">#sgu: embedding vector is shared across train, valid and test </span>
      <span style="color: #000000; background-color: #ffffff;">embedding</span> = tf.get_variable(
          <span style="color: #8b2252;">"embedding"</span>, [vocab_size, size], <span style="color: #000000; background-color: #ffffff;">dtype</span>=data_type())
      <span style="color: #000000; background-color: #ffffff;">inputs</span> = tf.nn.embedding_lookup(embedding, input_.input_data)

    <span style="color: #a020f0;">if</span> is_training <span style="color: #a020f0;">and</span> config.keep_prob &lt; <span style="color: #000000; background-color: #ffffff;">1</span>:
      <span style="color: #000000; background-color: #ffffff;">inputs</span> = tf.nn.dropout(inputs, config.keep_prob)

    <span style="color: #b22222;"># Simplified version of tensorflow.models.rnn.rnn.py's rnn().</span>
    <span style="color: #b22222;"># This builds an unrolled LSTM for tutorial purposes only.</span>
    <span style="color: #b22222;"># In general, use the rnn() or state_saving_rnn() from rnn.py.</span>
    <span style="color: #b22222;">#</span>
    <span style="color: #b22222;"># The alternative version of the code below is:</span>
    <span style="color: #b22222;">#</span>
    <span style="color: #b22222;"># inputs = tf.unstack(inputs, num=num_steps, axis=1)</span>
    <span style="color: #b22222;"># outputs, state = tf.nn.rnn(cell, inputs, initial_state=self._initial_state)</span>
    <span style="color: #000000; background-color: #ffffff;">outputs</span> = []
    <span style="color: #000000; background-color: #ffffff;">state</span> = <span style="color: #a020f0;">self</span>._initial_state
    <span style="color: #a020f0;">with</span> tf.variable_scope(<span style="color: #8b2252;">"RNN"</span>):
      <span style="color: #a020f0;">for</span> time_step <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(num_steps):
        <span style="color: #b22222;">#sgu: All LSTMs share the same weights/bias varibles.There are num_steps LSTM cells(MultiRNNCell); each MultiRNN has two layers. </span>
        <span style="color: #b22222;">#sgu: for valid/test, the reuse = True has been set and hence the if below has no effect. </span>
        <span style="color: #a020f0;">if</span> time_step &gt; <span style="color: #000000; background-color: #ffffff;">0</span>: tf.get_variable_scope().reuse_variables()
        (cell_output, state) = cell(inputs[:, time_step, :], state)
        outputs.append(cell_output)

    <span style="color: #b22222;">#sgu: cell_output: (batch_size, embedding_size) </span>
    <span style="color: #b22222;">#sgu: output shape: (num_steps*batch_size, embedding_size)</span>
    <span style="color: #000000; background-color: #ffffff;">output</span> = tf.reshape(tf.concat(<span style="color: #000000; background-color: #ffffff;">1</span>, outputs), [-<span style="color: #000000; background-color: #ffffff;">1</span>, size])
    <span style="color: #000000; background-color: #ffffff;">softmax_w</span> = tf.get_variable(
        <span style="color: #8b2252;">"softmax_w"</span>, [size, vocab_size], <span style="color: #000000; background-color: #ffffff;">dtype</span>=data_type())
    <span style="color: #000000; background-color: #ffffff;">softmax_b</span> = tf.get_variable(<span style="color: #8b2252;">"softmax_b"</span>, [vocab_size], <span style="color: #000000; background-color: #ffffff;">dtype</span>=data_type())
    <span style="color: #b22222;">#sgu: logit shape: (num_step*batch_size, vocab_size)</span>
    <span style="color: #000000; background-color: #ffffff;">logits</span> = tf.matmul(output, softmax_w) + softmax_b
    <span style="color: #000000; background-color: #ffffff;">loss</span> = tf.nn.seq2seq.sequence_loss_by_example(
        [logits],
        [tf.reshape(input_.targets, [-<span style="color: #000000; background-color: #ffffff;">1</span>])],
        [tf.ones([batch_size * num_steps], <span style="color: #000000; background-color: #ffffff;">dtype</span>=data_type())])
    <span style="color: #a020f0;">self</span>._cost = <span style="color: #000000; background-color: #ffffff;">cost</span> = tf.reduce_sum(loss) / batch_size
    <span style="color: #b22222;">#sgu: keep final state which is used as initial_state for next iteration(see run_epoch())</span>
    <span style="color: #a020f0;">self</span>._final_state = state  

    <span style="color: #a020f0;">if</span> <span style="color: #a020f0;">not</span> is_training:
      <span style="color: #a020f0;">return</span>

    <span style="color: #a020f0;">self</span>._lr = tf.Variable(<span style="color: #000000; background-color: #ffffff;">0</span>.<span style="color: #000000; background-color: #ffffff;">0</span>, <span style="color: #000000; background-color: #ffffff;">trainable</span>=<span style="color: #a020f0;">False</span>)
    <span style="color: #000000; background-color: #ffffff;">tvars</span> = tf.trainable_variables() 
    <span style="color: #000000; background-color: #ffffff;">grads</span>, <span style="color: #483d8b;">_</span> = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                      config.max_grad_norm)

    <span style="color: #b22222;">#sgu: When the learning rate self._lr changes, the optimizer picks up automatically  </span>
    <span style="color: #000000; background-color: #ffffff;">optimizer</span> = tf.train.GradientDescentOptimizer(<span style="color: #a020f0;">self</span>._lr)
    <span style="color: #a020f0;">self</span>._train_op = optimizer.apply_gradients(
        <span style="color: #483d8b;">zip</span>(grads, tvars),
        <span style="color: #000000; background-color: #ffffff;">global_step</span>=tf.contrib.framework.get_or_create_global_step())
    <span style="color: #a020f0;">self</span>._new_lr = tf.placeholder(
        tf.float32, <span style="color: #000000; background-color: #ffffff;">shape</span>=[], <span style="color: #000000; background-color: #ffffff;">name</span>=<span style="color: #8b2252;">"new_learning_rate"</span>)
    <span style="color: #a020f0;">self</span>._lr_update = tf.assign(<span style="color: #a020f0;">self</span>._lr, <span style="color: #a020f0;">self</span>._new_lr)  

  <span style="color: #a020f0;">def</span> <span style="color: #0000ff;">assign_lr</span>(<span style="color: #a020f0;">self</span>, session, lr_value):
    session.run(<span style="color: #a020f0;">self</span>._lr_update, <span style="color: #000000; background-color: #ffffff;">feed_dict</span>={<span style="color: #a020f0;">self</span>._new_lr: lr_value})
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Feeding the States of LSTM</h2>
<div class="outline-text-2" id="text-3">
<p>
The <code>run_epoch</code> run multiple iterations to pass the one full dataset. In each iteration, the stats of LSTM needs to be fed. Note that the last state of the current iterations are used as initial state of next iterations. Also, the learning rate are kept constant during one epoch.  
</p>
<div class="org-src-container">

<pre class="src src-python"><span style="color: #a020f0;">def</span> <span style="color: #0000ff;">run_epoch</span>(session, model, <span style="color: #000000; background-color: #ffffff;">eval_op</span>=<span style="color: #a020f0;">None</span>, <span style="color: #000000; background-color: #ffffff;">verbose</span>=<span style="color: #a020f0;">False</span>):
  <span style="color: #8b2252;">"""Runs the model on the given data."""</span>
  <span style="color: #000000; background-color: #ffffff;">start_time</span> = time.time()
  <span style="color: #000000; background-color: #ffffff;">costs</span> = <span style="color: #000000; background-color: #ffffff;">0</span>.<span style="color: #000000; background-color: #ffffff;">0</span>
  <span style="color: #000000; background-color: #ffffff;">iters</span> = <span style="color: #000000; background-color: #ffffff;">0</span>
  <span style="color: #000000; background-color: #ffffff;">state</span> = session.run(model.initial_state)

  <span style="color: #000000; background-color: #ffffff;">fetches</span> = {
      <span style="color: #8b2252;">"cost"</span>: model.cost,
      <span style="color: #8b2252;">"final_state"</span>: model.final_state,
  }
  <span style="color: #a020f0;">if</span> eval_op <span style="color: #a020f0;">is</span> <span style="color: #a020f0;">not</span> <span style="color: #a020f0;">None</span>:
    <span style="color: #000000; background-color: #ffffff;">fetches</span>[<span style="color: #8b2252;">"eval_op"</span>] = eval_op

  <span style="color: #a020f0;">for</span> step <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">range</span>(model.input.epoch_size):
    <span style="color: #000000; background-color: #ffffff;">feed_dict</span> = {}
    <span style="color: #b22222;">#sgu: use the final state of the current mini-batch as the initial state of the subsequent minibatch</span>
    <span style="color: #b22222;">#sgu: multiple LSTM cells can stack together. state[i] is the state of i-th cell.  </span>
    <span style="color: #a020f0;">for</span> i, (c, h) <span style="color: #a020f0;">in</span> <span style="color: #483d8b;">enumerate</span>(model.initial_state):
      <span style="color: #000000; background-color: #ffffff;">feed_dict</span>[c] = state[i].c
      <span style="color: #000000; background-color: #ffffff;">feed_dict</span>[h] = state[i].h

    <span style="color: #000000; background-color: #ffffff;">vals</span> = session.run(fetches, feed_dict)
    <span style="color: #000000; background-color: #ffffff;">cost</span> = vals[<span style="color: #8b2252;">"cost"</span>]
    <span style="color: #000000; background-color: #ffffff;">state</span> = vals[<span style="color: #8b2252;">"final_state"</span>] 

    <span style="color: #000000; background-color: #ffffff;">costs</span> += cost
    <span style="color: #000000; background-color: #ffffff;">iters</span> += model.input.num_steps
    <span style="color: #a020f0;">if</span> verbose <span style="color: #a020f0;">and</span> step % (model.input.epoch_size // <span style="color: #000000; background-color: #ffffff;">10</span>) == <span style="color: #000000; background-color: #ffffff;">10</span>:
      <span style="color: #b22222;">#sgu: the 1st : % of progress in current epoch;</span>
      <span style="color: #b22222;">#     the 2nd : perplexity</span>
      <span style="color: #b22222;">#     the 3rd:  words per sec  so far in the training </span>
      <span style="color: #a020f0;">print</span>(<span style="color: #8b2252;">"%.3f perplexity: %.3f speed: %.0f wps"</span> %
            (step * <span style="color: #000000; background-color: #ffffff;">1</span>.<span style="color: #000000; background-color: #ffffff;">0</span> / model.input.epoch_size, np.exp(costs / iters),
             iters * model.input.batch_size / (time.time() - start_time)))

  <span style="color: #a020f0;">return</span> np.exp(costs / iters)
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: <span class="timestamp-wrapper"><span class="timestamp">&lt;2017-01-16 Mon&gt;</span></span></p>
<p class="author">Author: sgu</p>
<p class="date">Created: 2017-01-17 Tue 00:12</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
