-----------------------
#+TITLE: Python Summary 
#+DATE: 
#+CATEGORY: Programming
-----------------------
* Name Convention: 
  The name convention recommended by Python Enhancement Proposals(PEP)(Cf. [[http://legacy.python.org/dev/peps/pep-0008/][PEP8: Style Guide to Python Code ]]) is quite different from the name convention adopted by C++ community. In particular, 
  ** mixCase is discouraged. 
  **  Any identifier in  a class of the form =__spam= (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with leading underscore(s) stripped(Cf. [[https://docs.python.org/2/tutorial/classes.html#private-variables-and-class-local-references][Pydoc: Private Variables and Class-local References ]]).
  ** Any identifier in a class of the form =__getitem__= is a special method(Cf. [[https://docs.python.org/2/reference/datamodel.html#special-method-names][Special Methods]])
  ** A method in a module whose name starts with an underscore is not exported by default(i.e., =__all__=  is not defined) when `from mymodule import *` is invoked. Even when =__all__= is set appropriately, it's still considered to be a good practice to prefix the internal interface (packages, modules, classes, functions, attributes or other names) with a single leading underscore (Cf. [[http://legacy.python.org/dev/peps/pep-0008/#public-and-internal-interfaces][PEP8: public and private interface]]). 

* Modules: 
** Related commands/variables
     + show all loaded modules: sys.modules
** Modules are first class objects. 
     + It's a common practice to import a module in another module initialization file =__init__.py=. For example, Pandas imports numpy and hence we can refer numpy as pandas.np. However, this is not visible to user.  So users could import numpy again. In this case, all imported numpy binds to the same object and hence have the same id(...).
** All statements in the source file are executed. 

** Use =__all__= = ['bar','foo'] to precisely control the set of names import by from /module/ import =*= 
  
** Use /from module import foo/
   This will load the specific definitions into a current namespace. However, the binding rules for variables.  The global namespace for a function  is always the module in which the function is defined.
  #+BEGIN_SRC python 
  from spam import foo
  a = 42
  foo() # Prints "I'm foo and a is 37"
  #+END_SRC  
  ** The use of =__name__=:
  Each module defines a variable =__name__= with the module's name. The programs can exam this variable to determine the module in which they are executing. The top level module of the interpreter is called =__main__=.Programs specified in the command line or entered interactively run inside =__main=. The following program prints "in mymodule" when being executed by 'import mymodule' and print 'in __main__ ' when executed by `python mymodule.py`
   #+BEGIN_SRC python 
     ## mymodule.py
     if __name__ == '__main__':
        print "in __main__"
	 else:
	    print "in %s" % (__name__,) 
   #+END_SRC
* OOP:
** Unlike C++/Java, Base class _init_() is not automatically invoked:
 #+BEGIN_SRC python
        class Base(object):
            def __init__(self):
                print "In Base::init"
                self.a=1
        class Derived(Base):
            def __init__(self):
                print "In Derive::init"
                self.b=1
            def myprint(self):
                print "a=%d" % self.a
        obj = Derived()
        obj.myprint()   #error AttributeError: 'Derived' object has no attribute 'a'

 #+END_SRC
** In Python, the instance's attribute can be deleted.
 #+BEGIN_SRC python 
   class MyObject:
       """
       Test some pythonic OO features. 
       """
       a = 10;
       def __init__(self):
           self.b = 10;
       def f(self):
           print "this is f"
       
   obj = MyObject()
   del obj.a #error: only instance attribute can be deleted. 
   del obj.f #same error as above
   del obj.b #correct. 
 #+END_SRC

** The use of "@property": Uniform Access Principle. 
* Type conversion
  Speicial Methods __int__(), __float__() ect. are called by explicit conversion such as int(x), float(x), but never called implicitly to perform type conversion during mix-type arithmetic. 

* MetaClass:
In the following code, the descriptor TypedProperty doesn't hold the value directly. Instead, it holds the name of itself in the class. How the TypedProperty knows its own name in the class it belongs to? The magic in the Meta class. The variable num is a member of class Foo. When Foo is created by the meta class TypedMeta, the metaclass __new__(..) method has a dictionary, which is a map betweem the member name and the memeber. TypedMeta.__new__(..) changes the num's field name to num's name suffixed by an underscore.  Now, basically, the descriptor simply becomes a *proxy* to the underlining field "_num" which is stored in Foo itself. TypedMeta also use the __slots__ to forbid adding any new field to the class. 
#+BEGIN_SRC python
class TypedProperty(object):
    def __init__(self, type, default=None):
        self.name = None
        self.type = type
        if default: 
            self.default = default
        else:
            self.default = type()
    def __get__(self, instance, cls):
        print "self.name",self.name
        return getattr(instance,self.name, self.default)
    def __set__(self, instance, value):
        if not isinstance(value, self.type):
            raise TypeError("Must be a %s" % self.type)
        setattr(instance, self.name, value)
    def __delete__(self, instance):
        raise AttributeError("cannot delete attribute")
class TypedMeta(type):
    def __new__(cls, name, bases, dict):
        slots=[]
        print "TypedMeta::__new__.cls=%s,name=%s" % (cls,name)
        for key, value in dict.items():
            if isinstance(value, TypedProperty):
                value.name = "_"+key  #bind the memeber variabls's name to the  the descriptor 
                slots.append(value.name)
        dict['__slots__']=slots
        return type.__new__(cls,name,bases,dict)
class Typed:
    __metaclass__ = TypedMeta

class Foo(Typed):
    name = TypedProperty(str)
    num =  TypedProperty(int,42)
f=Foo()
f.num #output self.name _num, 42
f._num # AttributeError: _num; _num doesn't exist, not yet. 
f.num=44 # the descriptor's __set__ method is called which creates a new field _num
f._num  #44, now _num exists after a call to f.num
#+END_SRC

* Scoping Rules and Closure
  + Variables in nested Functions are bound using /lexical scoping/. That is, names are resolved by first checking the local scope and then all enclosing scopres of the outer function definition from the innermost scope to the outermost scope. If no match is found, the global and built-in namespace are checked. Although names in enclosing scopes are accessible. Python 2 only allows variables to be reassigned in the innermost scope(local variables) and the global namespace(using global). Python 3 allows this by using nonlocal. 

  #+BEGIN_SRC python
  def countdown(n):
     def next():
       nonlocal n #only valid in python 3
       r=n
       n -= 1
       print "r=",r
     return next
  #in python 2.x , we can use list and dictionary to workaround. 
  def countdown(n):
    l=[n]
    def next():
       r=l
       r[0] -= 1
       print "r=",r
    return next
  #+END_SRC
  + When the statements that make up a function are packaged together with the environment in which they execute, the resulting object is known as closure. One use of closure is for lazy/delayed evaluation. Using this closure pattern is faster than using a class to capture the variables. 
  #+BEGIN_SRC python
  from urlib.request import urlopen(Python3)
  def page(url):
      def get():
	      return urlopen(url).read()
      return get
  python = page("http://www.python.org")
  >>> python 
  <function get at 0x95d5f0>
  jython = page("http://www.jython.org")
  >>> jython
  <function get at 0x9735f0>
  # the clousre is captured in the function object 
  python.__closure__[0].cell_contents
  'http://www.python.org'
  jython.__clousre__[0].cell_contents
  'http://www.jython.org'
  #+END_SRC

* Being Pythonic
** double asterisk
The double asterisk can be used in defining kwargs as well as passing a dict as arguments: 
#+BEGIN_SRC python 
args=dict(A=1, B=2);
setup(**args) ##equivalent to passing in A=1, B=2
#+END_SRC 

* Using Python for Text Parsing 
* NumPy
** view vs copy 
*** simply indexing return view and advance indexing returns copy 
#+BEGIN_SRC python 
 v2=v1[1:5]
 v2[0] = 1.0 #v1[1] is also changed
 v2 is v1[1:5] # return FALSE, the internal array is the same but meta data is not which make different identity See internal data structure for detail 
#+END_SRC 
** dtype-structured array
** syntax/function compared with matlab
- numpy.where() <--> find() in matlab
#+BEGIN_SRC python1
import numpy as np
data=np.zeros((2,), dtype=[('A','i4'),('B','f4'),('C','a10')])
data[:]=[(1,2.0,'Hello'),(2,3,'World')]
#+END_SRC
* Pandas
** Data Structure: 
*** Series
**** ndarry-like: can be passed to most NumPy function. Slicing also slice the index
**** dict-like, can set/get values by index label, like s['key']=val; However, unlike a python dict, if a label doesn't exist, an exception is raised. Note that dict-like insertion(with broadcasting) is ok with DataFrame, ie., df['new col']=val, df.loc[:,'C']=df.loc[:,'A'] can also perform enlargement. 
**** automatic label alignment 
#+BEGIN_SRC python
s[1:]+s[:-1] # due to the automatic label aligment, the first and the last entry is NaN
#+END_SRC
**** A pandas.series  can be mix-typed; this usually happen when we select a row by DataFrame.loc[label] which gives a Series of dtype: object 
#+BEGIN_SRC python
d={'C1':pd.Series([1.,2.,3.]),
   'C2':pd.Series(list('abc'))}
df=pd.DataFrame(d)
df.loc[0]
C1    1
C2    a
Name: 0, dtype: object
#+END_SRC
*** Data Frame
**** two axis: index(row labels, referred as df.index) and columns(column label,referred as df.columns)
**** can be created from a dict or Series,  numpy structured array, a list of dicts, a dict of tuples(multi-indexed)
#+BEGIN_SRC python
d={'C1':Series([1.,2.,3.,], index=['a','b','c']),
   'C2:'Series(...)}
df=DataFrame(d)
#+END_SRC
**** If the data in DataFrame is numeric, elementwise numpy function and various other numpy functions can be used with no issues.
**** There exists 3d(panel data in econometrics), 4d and nd dataframe.   
** Index/Selection
#+BEGIN_SRC python
df['foo'] #get the columne 'foo'
df.foo #column can be accessed like class attribute if the column name is a valid python variable name.
df[df<0] #two dimension indexing; the resulting dataframe  has the same shape (still two dimensions in this cases) but the selected entries are NaN; In R, this logical indexing in two dimensions results in one dimension vector. 
df.where(df>0, -df, inplace=TRUE) # ( replace negatives with its magnitudes. 
df.mask(df>0) # replace the positive entries with NaN
df.lookup([row1,row2],[col1,col2]) #return 1d numpy array with two entries (row1,col1),(row2,col2))
df.select(lambda x: x=='A', axis=1) #keep the column 'A' only 
#+END_SRC

**** Three indexing methods: by label (.loc), by position(.iloc)  and advance indexing (.ix) 
#+BEGIN_SRC python
df.loc[row_idx, col_idx]
panel.loc[item_idx, major_idx, minor_idx) #assume all for unspecified dim. 
df[colnames] #([] correponds __getitem__, selecting low-dim slice 
panel[itemnames]  
#+END_SRC

**** use at()/iat() methods for fast scalar value getting and setting([] is slow since it must handle a lot of cases) 
**** Gotchas 
+ df.loc[start_idx:last_idx] includes the last index but df[start_idx:last_idx] doesn't. 
** Chaining 
#+BEGIN_SRC python
iris=read_csv('data/iris.data')
iris.query('SepalLength > 5').assign(SepalRatio=lambda x: x.SepalWidth/x.SepalLength, PetalRatio=lambda x: x.PetalWidth/PetalLength).plot(kind='scatter', x='SepalRatio',y='PetalRatio') 
#+END_SRC
** Dataframe: apply
#+BEGIN_SRC python
   df.apply(lambda x,y: x.where(x>0,y), y=df['A']) #same as df.where(df>0,df['A'],axis='index',inplace=TRUE))
#+END_SRC
** display
#+BEGIN_SRC python
> set_option('display.width',160) #default is 80
> df.head()
> bassball=read_csv('data/baseball.csv')
> baseball.info()
> baseball.ilocal[-20:, :12].to_string()
#+END_SRC
** exploratory data analysis
#+BEGIN_SRC python
df.dtypes  #show the dtypes for each column
df.columns=[x.lower() for x in df.columns] #show column names in lower case; 
df.values #access the values #for heterogeneous data, dtype will be chosen to accommodate all data invloved.
df.get_dtype_counts() #return the number of columns of each type
df.someColumn.str.lower #vectorized string methods are available through string attribute. Note that for the column with string, the dtype is object 
df.convert_objects(convert_numeric=True) convert string that represents a numbedf.select_dtypes(include=['bool','int64']) #select the column of dtype bool and int64
df.someCol.isin([2,4,6]) #return boolean vectors to indicate whether someCol in {2,4,6}; this boolean vectors can furthur be used for slicing.
#+END_SRC
** Iterator 
#+BEGIN_SRC python 
for idx, row in df.iterrows(): 
     print ('%s\n%s' % (idx,row))
#row is a reference, change to row will also alter df. 
#+END_SRC
** Query
- A very succinct and convenient syntax 
#+BEGIN_SRC python
df.query('b==["a","b","c"]') #select rows where column b is the one of the specified values. 
df.query('b in a') #select rows where column b is one of the values in column a; which is equivalent to  df[df.b.isin(df.a)]
#+END_SRC
- A bug makes it extremely dangerous, 
#+BEGIN_SRC python
df.query("A='abc'")  # This reset the columan A! '=' should be '=='
#+END_SRC
** Accelerated operations
Pandas has support accelerating certain types of binary numerical and boolean operations using the numexpr and bottleneck libraries. 

** convenient features
Pandas has the following convenient features 
+ combining two overlapping dataset( A is more preferred but B has more data)
 
** Gotcha
*** Never specify the dtype when reading in data. Instead, first read in, then convert http://stackoverflow.com/questions/15210962/specifying-dtype-with-pandas-read-csv
#+BEGIN_SRC python
# dont' use dtype converters explicity for the columns you care about
# they will be converted to float64 if possible, or object if they cannot
df = pd.read_csv('test.csv'.....)

#### this is optional and related to the issue you posted ####
# force anything that is not a numeric to nan
# columns are the list of columns that you are interesetd in
df[columns] = df[columns].convert_objects(convert_numeric=True)


    # astype
    df[columns] = df[columns].astype('float32')

see http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion

Its not as efficient as doing it directly in read_csv (but that requires
#+END_SRC
* Pymssql 
** Doc: http://www.pymssql.org/en/latest/intro.html
** The parameter substitution of pymssql works sliently different than the python DB-API documented in https://code.google.com/p/pyodbc/wiki/Cursor. (Cf.http://stackoverflow.com/questions/23244450/cant-insert-tuple-to-mssql-db). 
#+BEGIN_SRC python
cursor.execute("select a from tbl where b=? and c=?", (10, 'Hello')) #Error; should change to: 
cursor.execute("select a from tbl where b=%d and c=%s", (10, 'Hello')) # string will be properly quoted. 
#+END_SRC
* matplotlib
If X-window is not setup correctly and matplotlib pylab is not working properly, try:
** Use inline
		%matplotlib inline
		pylab,plot(x,y)
		pylab.show()
** Use non-interactive backend:   
   import matplotlib
   matplotlib.use('Agg')  # this should put before pylab.plot()
   pylab.plot(x,y)
   pylab.savefig("test.png")
